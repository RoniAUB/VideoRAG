{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43884afb",
   "metadata": {},
   "source": [
    "# Downloading the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95e7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "def download_video(url, output_path):\n",
    "    \"\"\"\n",
    "    Download a video from a given URL using yt-dlp and save it to the output path.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the video to download.\n",
    "    output_path (str): The path to save the video to.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the metadata of the video.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ydl_opts = {\n",
    "            'outtmpl': f'{output_path}/input_vid.%(ext)s',\n",
    "            'format': 'bestvideo+bestaudio/best',\n",
    "        }\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(url, download=True)\n",
    "            metadata = {\n",
    "                \"Author\": info.get(\"uploader\"),\n",
    "                \"Title\": info.get(\"title\"),\n",
    "                \"Views\": info.get(\"view_count\"),\n",
    "            }\n",
    "            return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08a555",
   "metadata": {},
   "source": [
    "# Downloading the Video, the information and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c1410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecated Feature: Support for Python version 3.8 has been deprecated. Please update to Python 3.9 or above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: yt-dlp in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (2024.10.22)\n",
      "Requirement already satisfied: ffmpeg-python in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (0.2.0)\n",
      "Requirement already satisfied: brotli in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from yt-dlp) (1.1.0)\n",
      "Requirement already satisfied: certifi in c:\\program files\\spyder\\pkgs (from yt-dlp) (2024.6.2)\n",
      "Requirement already satisfied: mutagen in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from yt-dlp) (1.47.0)\n",
      "Requirement already satisfied: pycryptodomex in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from yt-dlp) (3.22.0)\n",
      "Requirement already satisfied: requests<3,>=2.32.2 in c:\\program files\\spyder\\pkgs (from yt-dlp) (2.32.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.17 in c:\\program files\\spyder\\pkgs (from yt-dlp) (2.2.1)\n",
      "Requirement already satisfied: websockets>=13.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from yt-dlp) (13.1)\n",
      "Requirement already satisfied: future in c:\\program files\\spyder\\python\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\spyder\\pkgs (from requests<3,>=2.32.2->yt-dlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\spyder\\pkgs (from requests<3,>=2.32.2->yt-dlp) (3.7)\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=dARr3lGKwk8\n",
      "[youtube] dARr3lGKwk8: Downloading webpage\n",
      "[youtube] dARr3lGKwk8: Downloading ios player API JSON\n",
      "[youtube] dARr3lGKwk8: Downloading mweb player API JSON\n",
      "[youtube] dARr3lGKwk8: Downloading player 8102da6c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/8102da6c/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] dARr3lGKwk8: nsig extraction failed: Some formats may be missing\n",
      "         n = UTdH8TtAOjSGACiR ; player = https://www.youtube.com/s/player/8102da6c/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/8102da6c/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] dARr3lGKwk8: nsig extraction failed: Some formats may be missing\n",
      "         n = 7n_UP1EDa_R4laJo ; player = https://www.youtube.com/s/player/8102da6c/player_ias.vflset/en_US/base.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] dARr3lGKwk8: Downloading m3u8 information\n",
      "[info] dARr3lGKwk8: Downloading 1 format(s): 137+251\n",
      "[download] Destination: Parameterized Complexity of token sliding, token jumping - Amer Mouawad [dARr3lGKwk8].f137.mp4\n",
      "[download] 100% of   77.76MiB in 00:00:23 at 3.24MiB/s     \n",
      "[download] Destination: Parameterized Complexity of token sliding, token jumping - Amer Mouawad [dARr3lGKwk8].f251.webm\n",
      "[download] 100% of   45.34MiB in 00:00:14 at 3.14MiB/s   \n",
      "[Merger] Merging formats into \"Parameterized Complexity of token sliding, token jumping - Amer Mouawad [dARr3lGKwk8].mkv\"\n",
      "Deleting original file Parameterized Complexity of token sliding, token jumping - Amer Mouawad [dARr3lGKwk8].f251.webm (pass -k to keep)\n",
      "Deleting original file Parameterized Complexity of token sliding, token jumping - Amer Mouawad [dARr3lGKwk8].f137.mp4 (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "# Define output template with metadata\n",
    "ydl_opts = {\n",
    "    'outtmpl': '%(title).100s - %(uploader)s - %(view_count)s views.%(ext)s',\n",
    "    # Limit title to 100 characters to avoid file system issues\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382b02f",
   "metadata": {},
   "source": [
    "# Video Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9c903",
   "metadata": {},
   "source": [
    "## Extract frame changes and their respective time-stamps using OpenAI SceneManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36192805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "VideoManager is deprecated and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 scenes detected.\n",
      "Saved scene 1 at 00:00:00.000\n",
      "Saved scene 2 at 00:00:00.680\n",
      "Saved scene 3 at 00:01:53.400\n",
      "Saved scene 4 at 00:03:16.880\n",
      "Saved scene 5 at 00:03:20.520\n",
      "Saved scene 6 at 00:04:50.120\n",
      "Saved scene 7 at 00:06:16.800\n",
      "Saved scene 8 at 00:08:08.440\n",
      "Saved scene 9 at 00:09:26.800\n",
      "Saved scene 10 at 00:09:46.840\n",
      "Saved scene 11 at 00:11:12.480\n",
      "Saved scene 12 at 00:12:00.680\n",
      "Saved scene 13 at 00:12:12.880\n",
      "Saved scene 14 at 00:12:48.240\n",
      "Saved scene 15 at 00:14:57.400\n",
      "Saved scene 16 at 00:16:18.040\n",
      "Saved scene 17 at 00:16:48.680\n",
      "Saved scene 18 at 00:17:40.200\n",
      "Saved scene 19 at 00:18:16.640\n",
      "Saved scene 20 at 00:18:55.440\n",
      "Saved scene 21 at 00:21:56.240\n",
      "Saved scene 22 at 00:22:48.680\n",
      "Saved scene 23 at 00:23:24.680\n",
      "Saved scene 24 at 00:23:45.840\n",
      "Saved scene 25 at 00:23:53.560\n",
      "Saved scene 26 at 00:25:05.600\n",
      "Saved scene 27 at 00:28:42.960\n",
      "Saved scene 28 at 00:29:10.120\n",
      "Saved scene 29 at 00:29:24.160\n",
      "Saved scene 30 at 00:30:00.680\n",
      "Saved scene 31 at 00:30:40.880\n",
      "Saved scene 32 at 00:30:54.200\n",
      "Saved scene 33 at 00:30:58.800\n",
      "Saved scene 34 at 00:31:03.920\n",
      "Saved scene 35 at 00:32:35.520\n",
      "Saved scene 36 at 00:34:43.800\n",
      "Saved scene 37 at 00:37:57.480\n",
      "Saved scene 38 at 00:38:16.720\n",
      "Saved scene 39 at 00:38:47.480\n",
      "Saved scene 40 at 00:39:09.720\n",
      "Saved scene 41 at 00:41:24.680\n",
      "Saved scene 42 at 00:41:50.080\n",
      "Saved scene 43 at 00:43:05.960\n",
      "Saved scene 44 at 00:44:35.200\n",
      "Saved scene 45 at 00:45:59.200\n",
      "Saved scene 46 at 00:48:05.560\n",
      "Saved scene 47 at 00:49:06.200\n",
      "Saved scene 48 at 00:49:06.960\n",
      "Saved scene 49 at 00:49:07.640\n",
      "Saved scene 50 at 00:49:14.120\n",
      "Saved scene 51 at 00:50:09.640\n",
      "Saved scene 52 at 00:50:15.240\n",
      "Saved scene 53 at 00:55:31.440\n",
      "Saved scene 54 at 00:55:32.120\n",
      "Saved scene 55 at 00:56:23.200\n",
      "Saved scene 56 at 01:00:03.800\n",
      "Saved scene 57 at 01:00:06.960\n",
      "Scene frames and timestamps saved in: scene_changes_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "from scenedetect.video_splitter import split_video_ffmpeg\n",
    "\n",
    "def save_scene_frames_with_timestamps(video_path, output_folder, threshold=5.0):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Setup SceneDetect\n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "    scene_manager.add_detector(ContentDetector(threshold=threshold))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "\n",
    "    # Detect scenes\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "\n",
    "    print(f\"{len(scene_list)} scenes detected.\")\n",
    "\n",
    "    # Open original video with OpenCV for frame extraction\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Save timestamps\n",
    "    timestamp_path = os.path.join(output_folder, \"timestamps.txt\")\n",
    "    with open(timestamp_path, \"w\") as timestamp_file:\n",
    "        for i, (start_time, _) in enumerate(scene_list):\n",
    "            # Convert start timecode to frame number\n",
    "            start_frame_num = int(start_time.get_frames())\n",
    "\n",
    "            # Seek to the frame\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                # Save frame\n",
    "                frame_filename = os.path.join(output_folder, f\"scene_{i+1:04d}.png\")\n",
    "                cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "                # Save timestamp\n",
    "                timestamp_file.write(f\"Scene {i+1:04d}: {start_time.get_seconds():.2f} seconds\\n\")\n",
    "                print(f\"Saved scene {i+1} at {start_time.get_timecode()}\")\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Scene frames and timestamps saved in: {output_folder}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = 'Parameterized Complexity of token sliding, token jumping - Amer Mouawad [dARr3lGKwk8].mkv'\n",
    "    output_folder = 'scene_changes_output'\n",
    "    save_scene_frames_with_timestamps(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d95d48",
   "metadata": {},
   "source": [
    "## Extract frame changes and their respective time-stamps by considering the frame contrast change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def calculate_frame_difference(prev_frame, current_frame):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared error between two frames.\n",
    "    \"\"\"\n",
    "    # Convert frames to grayscale\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Compute the absolute difference between the frames\n",
    "    difference = cv2.absdiff(prev_gray, current_gray)\n",
    "    \n",
    "    # Sum the differences to get a score\n",
    "    score = np.sum(difference)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def save_frames_with_timestamps(video_path, output_folder, threshold=1000000):\n",
    "    \"\"\"\n",
    "    Extracts frames from a video and saves each frame as an image in the specified folder,\n",
    "    while also saving the timestamp of each frame, only when a scene change is detected.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: The path to the input video file (MKV format).\n",
    "    - output_folder: The folder where the frames and timestamps will be saved.\n",
    "    - threshold: The difference threshold to detect scene changes (higher = less sensitivity).\n",
    "    \"\"\"\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Couldn't open video.\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Get frames per second of the video\n",
    "\n",
    "    # Read the first frame\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Couldn't read the first frame.\")\n",
    "        return\n",
    "\n",
    "    # Open a file to save timestamps\n",
    "    timestamp_filename = os.path.join(output_folder, \"timestamps.txt\")\n",
    "    with open(timestamp_filename, 'w') as timestamp_file:\n",
    "        while True:\n",
    "            ret, current_frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "\n",
    "            # Calculate the difference between the previous and current frame\n",
    "            frame_diff = calculate_frame_difference(prev_frame, current_frame)\n",
    "\n",
    "            # If the difference exceeds the threshold, consider it a scene change\n",
    "            if frame_diff > threshold:\n",
    "                # Calculate the timestamp for the current frame\n",
    "                timestamp = frame_count / fps  # Time in seconds\n",
    "\n",
    "                # Save the frame as an image (e.g., PNG format)\n",
    "                frame_filename = os.path.join(output_folder, f\"frame_{frame_count:04d}.png\")\n",
    "                cv2.imwrite(frame_filename, current_frame)\n",
    "\n",
    "                # Save the timestamp in the text file\n",
    "                timestamp_file.write(f\"Frame {frame_count:04d}: {timestamp:.4f} seconds\\n\")\n",
    "\n",
    "            # Update the previous frame to be the current frame\n",
    "            prev_frame = current_frame\n",
    "            frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Frames extraction completed. Timestamps saved to {timestamp_filename}.\")\n",
    "\n",
    "# Example usage:\n",
    "video_path = 'Parameterized Complexity of token sliding, token jumping - Amer Mouawad [dARr3lGKwk8].mkv'\n",
    "output_folder = 'frames_output_folder'\n",
    "save_frames_with_timestamps(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c5a95",
   "metadata": {},
   "source": [
    "## Seperating audio from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a58163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': True, 'metadata': {'COMPATIBLE_BRANDS': 'iso6avc1mp41', 'MAJOR_BRAND': 'dash', 'MINOR_VERSION': '0', 'ENCODER': 'Lavf62.0.102'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1920, 1080], 'bitrate': None, 'fps': 25.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'HANDLER_NAME': 'ISO Media file produced by Google Inc.', 'VENDOR_ID': '[0][0][0][0]', 'DURATION': '01:02:37.600000000'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': 'eng', 'default': True, 'fps': 48000, 'bitrate': None, 'metadata': {'Metadata': '', 'DURATION': '01:02:37.628000000'}}], 'input_number': 0}], 'duration': 3757.63, 'bitrate': 273, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1920, 1080], 'video_bitrate': None, 'video_fps': 25.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 48000, 'audio_bitrate': None, 'video_duration': 3757.63, 'video_n_frames': 93940}\n",
      "c:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -i Parameterized Complexity of token sliding, token jumping - Amer Mouawad [dARr3lGKwk8].mkv -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Writing audio in audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted and saved to audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy import VideoFileClip\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "filepath = 'Parameterized Complexity of token sliding, token jumping - Amer Mouawad [dARr3lGKwk8].mkv'\n",
    "output_audio_path = 'audio.wav'\n",
    "def extract_audio_from_video(video_path, output_audio_path):\n",
    "    \"\"\"\n",
    "    Extract audio from a video file and save it to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    video_path (str): The path to the video file.\n",
    "    output_audio_path (str): The path to save the extracted audio.\n",
    "\n",
    "    \"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio = clip.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    \n",
    "    clip.close()\n",
    "    print(f\"Audio extracted and saved to {output_audio_path}\")\n",
    "extract_audio_from_video(filepath, output_audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c0da7",
   "metadata": {},
   "source": [
    "## Transcribing the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88644b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing from file...\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "recognition request failed: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\speech_recognition\\recognizers\\google.py:215\u001b[39m, in \u001b[36mobtain_transcription\u001b[39m\u001b[34m(request, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     response = \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\urllib\\request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\urllib\\request.py:521\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    520\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\urllib\\request.py:630\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\urllib\\request.py:559\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    558\u001b[39m args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\urllib\\request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    491\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\urllib\\request.py:639\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 503: Service Unavailable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRequestError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 133\u001b[39m\n\u001b[32m    131\u001b[39m audio_path = \u001b[33m\"\u001b[39m\u001b[33maudio.wav\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m output_text_file = \u001b[33m\"\u001b[39m\u001b[33mtranscription3.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_text_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# For file input (provide the path to an audio file):\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# transcribe_audio(\"path_to_audio_file.wav\")\u001b[39;00m\n\u001b[32m    137\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# output_text_file = \"transcription2.txt\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# paragraphs = audio_to_text_faster_whisper(audio_path, output_text_file)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mtranscribe_audio\u001b[39m\u001b[34m(audio_path, output_text_file)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Recognize speech using Google's Web Speech API\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTranscribing from file...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m text = \u001b[43mrecognizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecognize_google\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTranscription: \u001b[39m\u001b[33m\"\u001b[39m + text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\speech_recognition\\recognizers\\google.py:255\u001b[39m, in \u001b[36mrecognize_legacy\u001b[39m\u001b[34m(recognizer, audio_data, key, language, pfilter, show_all, with_confidence, endpoint)\u001b[39m\n\u001b[32m    250\u001b[39m request_builder = create_request_builder(\n\u001b[32m    251\u001b[39m     endpoint=endpoint, key=key, language=language, filter_level=pfilter\n\u001b[32m    252\u001b[39m )\n\u001b[32m    253\u001b[39m request = request_builder.build(audio_data)\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m response_text = \u001b[43mobtain_transcription\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moperation_timeout\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m output_parser = OutputParser(\n\u001b[32m    260\u001b[39m     show_all=show_all, with_confidence=with_confidence\n\u001b[32m    261\u001b[39m )\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_parser.parse(response_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\speech_recognition\\recognizers\\google.py:217\u001b[39m, in \u001b[36mobtain_transcription\u001b[39m\u001b[34m(request, timeout)\u001b[39m\n\u001b[32m    215\u001b[39m     response = urlopen(request, timeout=timeout)\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\u001b[33m\"\u001b[39m\u001b[33mrecognition request failed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(e.reason))\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m URLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\n\u001b[32m    220\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrecognition connection failed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(e.reason)\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[31mRequestError\u001b[39m: recognition request failed: Service Unavailable"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "def audio_to_text_whisper(audio_path, output_text_file, gap_threshold=2.5):\n",
    "\n",
    "    \"\"\"\n",
    "    Transcribes audio using Whisper, chunks it into paragraphs based on silence gaps,\n",
    "    and saves each paragraph with the starting timestamp.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_path (str): Path to the input audio file.\n",
    "    - output_text_file (str): Path to save the paragraph-formatted transcription text.\n",
    "    - gap_threshold (float): Minimum gap (in seconds) to define a new paragraph.\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"base\",device=\"cuda\")\n",
    "\n",
    "    print(\"Transcribing audio with Whisper...\")\n",
    "    result = model.transcribe(audio_path, verbose=False, word_timestamps=False)\n",
    "\n",
    "    paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "    current_start = None\n",
    "    prev_end = None\n",
    "\n",
    "    for segment in result[\"segments\"]:\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        text = segment[\"text\"].strip()\n",
    "\n",
    "        # If this is the first segment or there's a long pause, start a new paragraph\n",
    "        if current_start is None or (start - prev_end) > gap_threshold:\n",
    "            if current_paragraph:\n",
    "                paragraphs.append((current_start, current_paragraph.strip()))\n",
    "            current_paragraph = text\n",
    "            current_start = start\n",
    "        else:\n",
    "            current_paragraph += \" \" + text\n",
    "\n",
    "        prev_end = end\n",
    "\n",
    "    # Append last paragraph\n",
    "    if current_paragraph:\n",
    "        paragraphs.append((current_start, current_paragraph.strip()))\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (timestamp, paragraph) in enumerate(paragraphs, 1):\n",
    "            minutes = int(timestamp // 60)\n",
    "            seconds = int(timestamp % 60)\n",
    "            time_str = f\"{minutes:02d}:{seconds:02d}\"\n",
    "            f.write(f\"[{time_str}] Paragraph {i}:\\n{paragraph}\\n\\n\")\n",
    "\n",
    "    print(f\"Paragraph transcription saved to {output_text_file}\")\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "\n",
    "def audio_to_text_faster_whisper(audio_path, output_text_file, gap_threshold=2.5, model_size=\"base\"):\n",
    "    \"\"\"\n",
    "    Transcribes audio using Faster-Whisper, chunks it into paragraphs based on silence gaps,\n",
    "    and saves each paragraph with the starting timestamp.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_path (str): Path to the input audio file.\n",
    "    - output_text_file (str): Path to save the paragraph-formatted transcription text.\n",
    "    - gap_threshold (float): Minimum gap (in seconds) to define a new paragraph.\n",
    "    - model_size (str): Model variant to use, e.g., \"base\", \"small\", \"medium\", \"large-v2\".\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
    "\n",
    "    print(f\"Loading Faster-Whisper model ({model_size}) on {device}...\")\n",
    "    model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
    "\n",
    "    print(\"Transcribing audio with Faster-Whisper...\")\n",
    "    segments, _ = model.transcribe(audio_path, word_timestamps=False)\n",
    "\n",
    "    paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "    current_start = None\n",
    "    prev_end = None\n",
    "\n",
    "    for segment in segments:\n",
    "        start = segment.start\n",
    "        end = segment.end\n",
    "        text = segment.text.strip()\n",
    "\n",
    "        if current_start is None or (start - prev_end) > gap_threshold:\n",
    "            if current_paragraph:\n",
    "                paragraphs.append((current_start, current_paragraph.strip()))\n",
    "            current_paragraph = text\n",
    "            current_start = start\n",
    "        else:\n",
    "            current_paragraph += \" \" + text\n",
    "\n",
    "        prev_end = end\n",
    "\n",
    "    # Append last paragraph\n",
    "    if current_paragraph:\n",
    "        paragraphs.append((current_start, current_paragraph.strip()))\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (timestamp, paragraph) in enumerate(paragraphs, 1):\n",
    "            minutes = int(timestamp // 60)\n",
    "            seconds = int(timestamp % 60)\n",
    "            time_str = f\"{minutes:02d}:{seconds:02d}\"\n",
    "            f.write(f\"[{time_str}] Paragraph {i}:\\n{paragraph}\\n\\n\")\n",
    "\n",
    "    print(f\"Paragraph transcription saved to {output_text_file}\")\n",
    "    return paragraphs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_path = \"audio.wav\"\n",
    "    output_text_file = \"transcription.txt\"\n",
    "    paragraphs = audio_to_text_whisper(audio_path, output_text_file)\n",
    "\n",
    "    output_text_file = \"transcription2.txt\"\n",
    "    paragraphs = audio_to_text_faster_whisper(audio_path, output_text_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d656d11",
   "metadata": {},
   "source": [
    "## embedding the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51411436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading timestamps...\n",
      "Embedding images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\open_clip\\factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving FAISS index...\n",
      "Saving metadata...\n",
      "Done. 57 images processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def load_timestamps(timestamp_file):\n",
    "    timestamps = {}\n",
    "    with open(timestamp_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\": \")\n",
    "            if len(parts) == 2:\n",
    "                frame_id = parts[0].replace(\"Frame \", \"\").zfill(4)\n",
    "                timestamp = float(parts[1].split()[0])\n",
    "                timestamps[f\"frame_{frame_id}.png\"] = timestamp\n",
    "    return timestamps\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "def embed_images(image_folder, timestamps):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith(\".png\")])\n",
    "\n",
    "    for image_name in image_files:\n",
    "        image_path = os.path.join(image_folder, image_name)\n",
    "        try:\n",
    "            image_tensor = preprocess_image(image_path, preprocess).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_embedding = model.encode_image(image_tensor).cpu().numpy().flatten()\n",
    "\n",
    "            ts = timestamps.get(image_name, -1)\n",
    "            metadata.append({\n",
    "                \"filename\": image_name,\n",
    "                \"timestamp\": ts\n",
    "            })\n",
    "            embeddings.append(image_embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {image_name}: {e}\")\n",
    "\n",
    "    if not embeddings:\n",
    "        raise ValueError(\"No image embeddings were generated. Check image input and model compatibility.\")\n",
    "\n",
    "    return np.vstack(embeddings).astype(\"float32\"), metadata\n",
    "\n",
    "\n",
    "def save_faiss_index(embeddings, index_path):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "\n",
    "def save_metadata(metadata, metadata_path):\n",
    "    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "\n",
    "def build_image_rag_database(image_folder, timestamp_file, index_output_path=\"image_rag_index.faiss\", metadata_output_path=\"image_metadata.json\"):\n",
    "    print(\"Loading timestamps...\")\n",
    "    timestamps = load_timestamps(timestamp_file)\n",
    "\n",
    "    print(\"Embedding images...\")\n",
    "    embeddings, metadata = embed_images(image_folder, timestamps)\n",
    "\n",
    "    print(\"Saving FAISS index...\")\n",
    "    save_faiss_index(embeddings, index_output_path)\n",
    "\n",
    "    print(\"Saving metadata...\")\n",
    "    save_metadata(metadata, metadata_output_path)\n",
    "\n",
    "    print(f\"Done. {len(metadata)} images processed.\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_folder = \"scene_changes_output\"\n",
    "    timestamp_file = os.path.join(image_folder, \"timestamps.txt\")\n",
    "    build_image_rag_database(image_folder, timestamp_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa43fa",
   "metadata": {},
   "source": [
    "## Embedding the text by chunking each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1192b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00 (384,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "def load_transcription_paragraphs(filepath):\n",
    "    \"\"\"\n",
    "    Loads transcription paragraphs from a file with the format:\n",
    "    [MM:SS] Paragraph n:\\n<text>\\n\\n\n",
    "\n",
    "    Returns:\n",
    "        List of (timestamp_str, text) tuples.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    entries = re.findall(r\"\\[(\\d{2}:\\d{2})\\] Paragraph \\d+:\\n(.+?)(?=\\n\\n|\\Z)\", content, re.DOTALL)\n",
    "    for timestamp, text in entries:\n",
    "        paragraphs.append((timestamp.strip(), text.strip()))\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def embed_with_overlap(paragraphs, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", window_size=3, stride=1):\n",
    "    \"\"\"\n",
    "    Embeds text with overlapping windows.\n",
    "\n",
    "    Parameters:\n",
    "    - paragraphs (list): List of (timestamp, text) tuples.\n",
    "    - model_name (str): SentenceTransformer model name.\n",
    "    - window_size (int): Number of paragraphs per window.\n",
    "    - stride (int): Number of paragraphs to slide forward each step.\n",
    "\n",
    "    Returns:\n",
    "    - List of dictionaries with 'start_time', 'end_time', 'text', 'embedding'.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(paragraphs) - window_size + 1, stride):\n",
    "        chunk = paragraphs[i:i + window_size]\n",
    "        timestamps = [p[0] for p in chunk]\n",
    "        texts = [p[1] for p in chunk]\n",
    "        full_text = \" \".join(texts)\n",
    "\n",
    "        emb = model.encode(full_text)\n",
    "        embeddings.append({\n",
    "            \"start_time\": timestamps[0],\n",
    "            \"end_time\": timestamps[-1],\n",
    "            \"text\": full_text,\n",
    "            \"embedding\": emb\n",
    "        })\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def embed_with_phrase_overlap(paragraphs, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", window_size=5, overlap=2):\n",
    "    \"\"\"\n",
    "    Embeds transcription text with overlapping windows.\n",
    "    Each window includes the last `overlap` phrases from the previous window.\n",
    "\n",
    "    Parameters:\n",
    "    - paragraphs (list): List of (timestamp, text) tuples.\n",
    "    - model_name (str): SentenceTransformer model name.\n",
    "    - window_size (int): Total number of phrases per window.\n",
    "    - overlap (int): Number of overlapping phrases between windows.\n",
    "\n",
    "    Returns:\n",
    "    - List of dicts with 'start_time', 'end_time', 'text', 'embedding'.\n",
    "    \"\"\"\n",
    "    assert overlap < window_size, \"Overlap must be smaller than window size\"\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = []\n",
    "\n",
    "    step = window_size - overlap\n",
    "    i = 0\n",
    "\n",
    "    while i < len(paragraphs):\n",
    "        chunk = paragraphs[i:i + window_size]\n",
    "        if len(chunk) == 0:\n",
    "            break\n",
    "\n",
    "        timestamps = [p[0] for p in chunk]\n",
    "        texts = [p[1] for p in chunk]\n",
    "        full_text = \" \".join(texts)\n",
    "\n",
    "        emb = model.encode(full_text)\n",
    "        embeddings.append({\n",
    "            \"start_time\": timestamps[0],\n",
    "            \"end_time\": timestamps[-1],\n",
    "            \"text\": full_text,\n",
    "            \"embedding\": emb\n",
    "        })\n",
    "\n",
    "        i += step\n",
    "\n",
    "    return embeddings\n",
    "filepath = \"transcription.txt\"\n",
    "paragraphs = load_transcription_paragraphs(filepath)\n",
    "overlap_embeddings = embed_with_phrase_overlap(paragraphs, window_size=5, overlap=2)\n",
    "\n",
    "print(overlap_embeddings[0][\"start_time\"], overlap_embeddings[0][\"embedding\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7b94b",
   "metadata": {},
   "source": [
    "# Byuilding the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_and_group_srt_with_overlap(srt_file, group_size=5, overlap_size=2):\n",
    "    pattern = re.compile(r'\\d+\\s*\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\s*\\n(.*?)\\n(?=\\d+\\n|\\Z)', re.DOTALL)\n",
    "    with open(srt_file, 'r', encoding='utf-8') as f:\n",
    "        srt_content = f.read()\n",
    "    matches = pattern.findall(srt_content)\n",
    "\n",
    "    # Extract and clean text lines + timestamps\n",
    "    timestamps = [(start, end) for start, end, _ in matches]\n",
    "    phrases = [text.replace('\\n', ' ').strip() for _, _, text in matches]\n",
    "\n",
    "    # Group with overlap\n",
    "    paragraphs = []\n",
    "    paragraph_timestamps = []\n",
    "    i = 0\n",
    "    while i < len(phrases):\n",
    "        group = \" \".join(phrases[i:i + group_size])\n",
    "        group_timestamps = timestamps[i:i + group_size]\n",
    "\n",
    "        if group_timestamps:\n",
    "            start_time = group_timestamps[0][0]\n",
    "            end_time = group_timestamps[-1][1]\n",
    "            paragraph_timestamps.append((start_time, end_time))\n",
    "        else:\n",
    "            paragraph_timestamps.append((\"00:00:00,000\", \"00:00:00,000\"))\n",
    "\n",
    "        paragraphs.append(group)\n",
    "        i += group_size - overlap_size  # slide forward with overlap\n",
    "\n",
    "    return paragraphs, paragraph_timestamps\n",
    "\n",
    "def embed_and_save(paragraphs, timestamps=None, model_name='all-MiniLM-L6-v2', \n",
    "                   index_path='hnsw_index.bin', embeddings_path='embeddings.npy', \n",
    "                   timestamps_path='timestamps.npy', faiss_index_path='index.faiss'):\n",
    "    # Initialize model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = model.encode(paragraphs, show_progress_bar=True)\n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    # HNSWlib index\n",
    "    dim = embeddings.shape[1]\n",
    "    num_elements = embeddings.shape[0]\n",
    "    index = hnswlib.Index(space='cosine', dim=dim)\n",
    "    index.init_index(max_elements=num_elements, ef_construction=200, M=16)\n",
    "    index.add_items(embeddings, np.arange(num_elements))\n",
    "    index.save_index(index_path)\n",
    "    print(f\"HNSWlib index saved to: {index_path}\")\n",
    "\n",
    "    # Save raw embeddings\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    print(f\"Embeddings saved to: {embeddings_path}\")\n",
    "\n",
    "    # Save timestamps\n",
    "    if timestamps is not None:\n",
    "        np.save(timestamps_path, np.array(timestamps))\n",
    "        print(f\"Timestamps saved to: {timestamps_path}\")\n",
    "\n",
    "    # FAISS index (inner product, equivalent to cosine after normalization)\n",
    "    faiss_index = faiss.IndexFlatIP(dim)\n",
    "    faiss_index.add(embeddings)\n",
    "    faiss.write_index(faiss_index, faiss_index_path)\n",
    "    print(f\"FAISS index saved to: {faiss_index_path}\")\n",
    "\n",
    "    return index, embeddings\n",
    "\n",
    "# Process Whisper + all-MiniLM-L6-v2\n",
    "paragraphs1, timestamps1 = load_and_group_srt_with_overlap('Whisper.srt',  group_size=5, overlap_size=2)\n",
    "embed_and_save(paragraphs1, timestamps1, model_name='all-MiniLM-L6-v2',\n",
    "               index_path='hnsw_Whisper_all-MiniLM-L6-v2.bin',\n",
    "               embeddings_path='Whisper_all-MiniLM-L6-v2_embeddings.npy',\n",
    "               timestamps_path='Whisper_all-MiniLM-L6-v2_timestamps.npy',\n",
    "               faiss_index_path='faiss_Whisper_all-MiniLM-L6-v2.index')\n",
    "\n",
    "# Process Faster Whisper + all-MiniLM-L6-v2\n",
    "paragraphs2, timestamps2 = load_and_group_srt_with_overlap('Faster_Whisper.srt',  group_size=5, overlap_size=2)\n",
    "embed_and_save(paragraphs2, timestamps2, model_name='all-MiniLM-L6-v2',\n",
    "               index_path='hnsw_Faster_Whisper_all-MiniLM-L6-v2.bin',\n",
    "               embeddings_path='Faster_Whisper_all-MiniLM-L6-v2_embeddings.npy',\n",
    "               timestamps_path='Faster_Whisper_all-MiniLM-L6-v2_timestamps.npy',\n",
    "               faiss_index_path='faiss_Faster_Whisper_all-MiniLM-L6-v2.index')\n",
    "\n",
    "# Process Whisper + multi-qa-MiniLM-L6-cos-v1\n",
    "paragraphs1, timestamps1 = load_and_group_srt_with_overlap('Whisper.srt',  group_size=5, overlap_size=2)\n",
    "embed_and_save(paragraphs1, timestamps1, model_name='multi-qa-MiniLM-L6-cos-v1',\n",
    "               index_path='hnsw_Whisper_multi-qa-MiniLM-L6-cos-v1_embeddings.bin',\n",
    "               embeddings_path='Whisper_multi-qa-MiniLM-L6-cos-v1_embeddings.npy',\n",
    "               timestamps_path='Whisper_multi-qa-MiniLM-L6-cos-v1_timestamps.npy',\n",
    "               faiss_index_path='faiss_Whisper_multi-qa-MiniLM-L6-cos-v1.index')\n",
    "\n",
    "# Process Faster Whisper + multi-qa-MiniLM-L6-cos-v1\n",
    "paragraphs2, timestamps2 = load_and_group_srt_with_overlap('Faster_Whisper.srt',  group_size=5, overlap_size=2)\n",
    "embed_and_save(paragraphs2, timestamps2, model_name='multi-qa-MiniLM-L6-cos-v1',\n",
    "               index_path='hnsw_Faster_Whisper_multi-qa-MiniLM-L6-cos-v1_embeddings.bin',\n",
    "               embeddings_path='Faster_Whisper_multi-qa-MiniLM-L6-cos-v1_embeddings.npy',\n",
    "               timestamps_path='Faster_Whisper_multi-qa-MiniLM-L6-cos-v1_timestamps.npy',\n",
    "               faiss_index_path='faiss_Faster_Whisper_multi-qa-MiniLM-L6-cos-v1.index')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004921c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f148c71",
   "metadata": {},
   "source": [
    "# Fine_tuning a small LLM (TinyLLaMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29e2f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bc4886c1194fd09247566157a74a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10912\\869984196.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 01:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.821100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load your JSON file\n",
    "with open(\"Data_fine_tuning.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Format into prompt-completion pairs\n",
    "def format_example(example):\n",
    "    prompt = f\"Q: {example['question']}\\nA:\"\n",
    "    completion = f\" {example['answer']} {example['timestamp']}\"\n",
    "    return {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "formatted_data = [format_example(item) for item in data]\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT2 doesn't have pad token by default\n",
    "\n",
    "def tokenize(batch):\n",
    "    prompts = [p + c for p, c in zip(batch[\"prompt\"], batch[\"completion\"])]\n",
    "    return tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned-distilgpt2\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "def ask_question(question):\n",
    "    input_text = f\"Q: {question}\\nA:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_length=100, do_sample=True)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99408b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who is the Speaker?\n",
      "A: First, the Speaker of Congress is from an institution where the speaker is elected. Second, the speaker is chosen by a vote in Congress within 8-10 minutes. Third, the speaker is elected in an hour by minutes by a vote or 40 minutes (depending on the length of the speaker). 4:405:30:30:00 C-tailed test: 2:155:30:55:00 C-tailed test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which university is he from?\n",
      "A: It is the University of Southern Illinois, University of Colorado and the University of Oregon. The research was conducted in 2010 at the University of Georgia and was conducted as part of an ongoing work on the topic of the role of cross-hairs in the problem problem.[7]\n",
      "A: What is the role of cross-hairs in problem problems?\n",
      "A: The result: Cross-hairs in problems is relatively similar to the problem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: does the video mention any details about lightsabers?\n",
      "A: Yes, we mentioned these features, but did not mention other details about it. For example, when moving a circle with a circle on the graph, the feature clearly states they have different objects and can be seen in 23 diagrams.\n",
      "A: What was omitted with the image is that the same triangles are a bit larger and 3-yield triangles?\n",
      "A: Yes, the idea of a two-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who is the emperor?\n",
      "A: Yes, it is known that the emperor is king (Q3:2832). One could see in the book that the emperor was king of kingdoms of the kings brother, as opposed to those two members and the queen itself, in which case the emperor was king. 53:5855:1343:4056:0057:15:5558:10:0053:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the main topic of the video?\n",
      "A: The video can be used to teach about the problem solving: what is the problem? (A:00:064:594:304:304:404:504:454:504:504:504:504:504:504:504:504:504:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: If unsurem answer with I don't know, What is the main topic of the video?\n",
      "A: No, yes, yes and no: 1:002:40:30-3:00:001:00:00:302:00:00:303:00:303:00:00:304:00:00:005:00:00:00:00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the topic of discussion?\n",
      "A: The topic of topic will focus on the question about how to make decisions according to what is important in complex problems. The topic will focus on a particular topic, or a common question, and the following topic will highlight a specific topic: what is related to a problem: why does \"mathematical reduction\" be one of the main areas of problem solving problem?\n",
      "A: No, the topic will focus on several topics: a more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who let the dogs out?\n",
      "A: The most common problem is that dogs may not do the job due to a fact that the dog is unable to move at least 50 yards or 100 yards. This is solved by a number of problems using simple trigonometric calculations or by finding out if a second dog can move the same length and it is easy to find out that the second dog has not moved from the same distance. 3:404:555:00:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is Token sliding?\n",
      "A: Token slide is a very fast, long, and easily obtainable (often 104025-minute). Token slide slides are relatively portable, and allow the use of a simple table (4050 kHz).\n",
      "A: What is the time period of token slide slides being shown in FIGU 123946?\n",
      "A: Token slide is a very fast, full-time, short-lived, and short-lived,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is Token jumping?\n",
      "A: Token Jump describes jumping which is a jumping move, which means that token jumping is only allowed for movement on a given field. More on Token Jump at http://github.com/miller2hq/token-jump-pigging-over-lucky-step-0/3/ (Zanovsky)\n",
      "A: Token Jump describes jumping and how tokens can be fixed and how to move on a space-space-free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the main topic of the video?\n",
      "A: This is an answer to an important question. For example, for instance, this is a question on a computer screen that contains 4 bytes of memory. Similarly, for instance, in the video, this question has been presented to the audience, and thus has been a problem of solving the problem. Thus the problem is explained in the following sentence.\n",
      "A: This question is related to the problem of solving the problem of problem with\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who is the speaker, and from which university does he come from?\n",
      "A: This speaker is a student paper and can be used to estimate the figure size, and is a speaker. It will be helpful if the question arises. A: What is a student paper? A: It is a paper, and will be available for discussion. A: How does an audience size measure the figure size, and how can students describe it? A: Students can describe the figure size, and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: If unsure, answer with 'I don't know'. What is the main topic of the video?\n",
      "A: There is a discussion on the topic above where people can answer about whether the concept: [5](1) => [2](1) => [3] => [4] => [5] => [6] => [7] => [8] => [9] => [10] => [10] => [11] => [12] => [13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: does this video mention Batman's true Identity?\n",
      "A: Yes it. Yes, it was that Batman's identity, but not the two, was different.\n",
      "A: Yes. This was a small-scale level design exercise, which was done in the presence of blacking out every sequence. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (13) (14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which puzzle did this video mention?\n",
      "A: The answer is: NoNoNoAllOneOneoneoneoneoneoneoneone oneoneoneoneoneone13243454556/4:14:14:15:15:15:1\n",
      "Q: Which character is stronger according to the video, bombardino crocodile or tralalero tralala\n",
      "A: No, there is strong resistance between the two characters and a fixed set of polygons (45:507:305:50:005:00  4:60:00:00 4:00:004:00:00 4:50:00:00 4:50:004:\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"Who is the Speaker?\")\n",
    "ask_question(\"Which university is he from?\")\n",
    "ask_question(\"does the video mention any details about lightsabers?\")\n",
    "ask_question(\"Who is the emperor?\")\n",
    "ask_question(\"What is the main topic of the video?\")\n",
    "ask_question(\"If unsurem answer with I don't know, What is the main topic of the video?\")\n",
    "ask_question(\"What is the topic of discussion?\")\n",
    "ask_question(\"Who let the dogs out?\")\n",
    "ask_question(\"What is Token sliding?\")\n",
    "ask_question(\"What is Token jumping?\")\n",
    "ask_question(\"What is the main topic of the video?\")\n",
    "ask_question(\"Who is the speaker, and from which university does he come from?\")\n",
    "ask_question(\"If unsure, answer with 'I don't know'. What is the main topic of the video?\")\n",
    "ask_question(\"does this video mention Batman's true Identity?\")\n",
    "ask_question(\"Which puzzle did this video mention?\")\n",
    "ask_question(\"Which character is stronger according to the video, bombardino crocodile or tralalero tralala\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac368d",
   "metadata": {},
   "source": [
    "# Generating the .srt file (with the purpose of loading it as a subtitle though it did not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe524e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\whisper\\__init__.py:69: UserWarning: C:\\Users\\Administrator\\.cache\\whisper\\base.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(\n",
      "100%|| 139M/139M [06:30<00:00, 372kiB/s]\n",
      "c:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\whisper\\transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "c:\\Users\\Administrator\\miniconda3\\envs\\torch-cuda\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:08.160]  So, hello everyone. Welcome to the PC Seminar. Today we have this Professor Amir Mohad from\n",
      "[00:08.160 --> 00:13.040]  American University of Beirut and he'll be talking on the GERT and Parametri's complex\n",
      "[00:13.040 --> 00:18.080]  city of token sliding and token jumping. Thank you for joining us, Professor. Over to you now.\n",
      "[00:19.440 --> 00:23.760]  Thank you, President. Thank you for having me. It's a real pleasure to be here.\n",
      "[00:23.760 --> 00:32.640]  So, all right, let's jump right into it. So, since I did not really know the audience too well,\n",
      "[00:32.640 --> 00:39.920]  I made the assumption that many of you maybe have not seen this area of combinatorial\n",
      "[00:39.920 --> 00:46.400]  reconfiguration problems. So, I decided what I'm going to do is I'm going to give a gentle introduction\n",
      "[00:46.400 --> 00:51.920]  to the area just to show you how many exciting problems and open problems are there.\n",
      "[00:52.800 --> 00:59.520]  And then I will talk more about token jumping and token sliding, specifically what we know about them,\n",
      "[00:59.520 --> 01:05.840]  what we knew about them before we started working on this project, what we managed to discover\n",
      "[01:05.840 --> 01:11.840]  and the tons of questions that remain to be answered. Right, and it's a really,\n",
      "[01:13.600 --> 01:19.280]  I mean, the questions are so nice to state, so easy to state, and they are accessible really to\n",
      "[01:19.360 --> 01:25.600]  researchers at any level, which is one of the reasons why I enjoy working on these problems. So,\n",
      "[01:25.600 --> 01:32.800]  so hopefully you'll get to enjoy them too. So, before I start, I should point out that this is\n",
      "[01:32.800 --> 01:40.480]  joint work that started back in the combinatorial reconfiguration workshop almost two years ago.\n",
      "[01:42.160 --> 01:47.440]  And it's joint work with Valentine Bart, Nicolabuske, Le Mandalard, and Karl Lomer,\n",
      "[01:47.440 --> 01:57.680]  who is my master's student. All right, so the outline of the talk, it's going to be in four sections.\n",
      "[01:58.960 --> 02:04.640]  I will give a gentle introduction to combinatorial reconfiguration, because I know many of you might not\n",
      "[02:04.640 --> 02:12.960]  have seen such problems. Then I will talk about token jumping and token sliding, what we know about\n",
      "[02:13.120 --> 02:19.360]  them in terms of classical complexity or one-dimensional complexity. Then I'll talk about the\n",
      "[02:19.360 --> 02:24.720]  parameter as complexity of these two problems and what we know as of today, as we speak,\n",
      "[02:25.520 --> 02:31.120]  and what are the problems that remain to be solved. And then the last part of the lecture is where\n",
      "[02:31.120 --> 02:37.200]  I will put some of the technical stuff to show you to give you an idea about how we prove things\n",
      "[02:37.200 --> 02:42.640]  when we deal with such problems and where are the difficulties and what kind of techniques\n",
      "[02:42.640 --> 02:48.480]  have been developed. So I tried to keep the technical part as light as I could so that really we,\n",
      "[02:48.480 --> 02:56.400]  I mean, I can focus on the big picture and the questions to be asked and answered. So if you have\n",
      "[02:56.400 --> 03:02.400]  any questions along the way, please feel free to interrupt me either in the chat or by unmuting\n",
      "[03:02.400 --> 03:09.840]  yourselves. So don't worry about leaving the questions till the end. You can interrupt me whenever\n",
      "[03:09.840 --> 03:14.720]  you feel, whenever I say something that doesn't make sense. Hopefully that won't happen too often.\n",
      "[03:16.880 --> 03:23.360]  All right, so what is combinatorial configuration? So the best way I think to introduce is with a\n",
      "[03:23.360 --> 03:30.560]  familiar example, which is one player games and the most common one that we use is the 15 puzzle\n",
      "[03:30.560 --> 03:36.240]  game. So for those of you who don't know the 15 puzzle games, so you're given like a 4x4 grid\n",
      "[03:37.040 --> 03:43.520]  and you have one empty square and basically you have all the remaining 15 squares are numbered\n",
      "[03:43.520 --> 03:50.640]  from 1 to 15 and they come in some ordering and your job is to basically move the squares around\n",
      "[03:50.640 --> 03:57.760]  so that all the numbers become ordered. So it's a by-ro so they have to be ordered this way. So if you\n",
      "[03:57.760 --> 04:04.400]  notice in this figure, the only problem is that 14 and 15 are reversed but the only moves that\n",
      "[04:04.400 --> 04:12.560]  you're allowed to do is to basically move a number into the empty square. And basically you have\n",
      "[04:12.560 --> 04:18.880]  to do a sequence of moves so that you get all of the numbers in order. And for those of you who know\n",
      "[04:18.880 --> 04:26.960]  this game, this example that I have on the slide is actually unsolved. There is no way you can\n",
      "[04:26.960 --> 04:34.000]  flip the order in 15 of 14 and 15 in this puzzle. And I have a link here if you want to actually\n",
      "[04:34.000 --> 04:40.880]  play the puzzle online which is pretty fun. So why do I do I start my talk by talking about\n",
      "[04:40.880 --> 04:47.360]  15 puzzle? It's because it's really I mean the way you solve the 15 puzzle tells you a lot about\n",
      "[04:47.920 --> 04:55.280]  the area of combinatorial reconfiguration. So the standard way we would think about the 15 puzzle\n",
      "[04:55.280 --> 05:00.480]  is by looking at the state space or what we call the reconfiguration graph of the 15 puzzle.\n",
      "[05:01.280 --> 05:07.840]  So what does that graph consist of? Well, we have one vertex or one node in this graph\n",
      "[05:07.840 --> 05:14.320]  for each possible configuration of the puzzle. So basically each possible configuration so it\n",
      "[05:14.320 --> 05:19.680]  would be a possible permutation of the 15 numbers in addition to where you're going to put the empty\n",
      "[05:19.680 --> 05:27.760]  square. Each one of those will be a vertex in the graph. And now we connect two vertices in that\n",
      "[05:27.760 --> 05:34.000]  graph whenever one can be reached from the other by a single move. And what do we mean here by a\n",
      "[05:34.000 --> 05:41.360]  single mover? It's basically just moving a number into the empty square. So if you look at the top\n",
      "[05:41.360 --> 05:47.600]  node here in this graph, there are four possibilities that you can do in one move which we call\n",
      "[05:47.600 --> 05:53.040]  a reconfiguration step which is you can move nine into the empty square. You can move three into\n",
      "[05:53.040 --> 06:00.320]  the empty square, 12 or 15. And that gives us basically four neighbors of that vertex in the graph.\n",
      "[06:01.680 --> 06:07.440]  Okay, and we call this whole graph the reconfiguration graph or the state space if you're more\n",
      "[06:07.440 --> 06:15.440]  comfortable thinking about states, the states of the game. So now given this graph, the reconfiguration\n",
      "[06:15.440 --> 06:21.200]  graph, there are tons of very interesting questions that you can ask about it. There are structural\n",
      "[06:21.200 --> 06:26.960]  questions and there are algorithms to make questions. And these are typically the types of questions\n",
      "[06:26.960 --> 06:35.040]  that we're interested in in this area of combinatorial reconfiguration. So a couple of examples of\n",
      "[06:35.040 --> 06:41.040]  structural questions would be, well, the simplest one would be how big is this reconfiguration graph?\n",
      "[06:41.040 --> 06:48.480]  Right? How many vertices or how many edges? And that's usually not a very hard question to answer\n",
      "[06:48.560 --> 06:54.640]  in terms of upper and lower ones. More interestingly, you could ask, is this reconfiguration graph\n",
      "[06:54.640 --> 07:01.680]  connected? Right? Or is, can I reach any state starting from any other state by a sequence of\n",
      "[07:01.680 --> 07:08.800]  legal moves? And as I told you before, for the 15 puzzle, the reconfiguration graph is definitely\n",
      "[07:08.800 --> 07:15.040]  not connected because there was no way to reverse 14 and 15 in the previous example that I showed\n",
      "[07:15.120 --> 07:20.400]  you and you can easily prove that, by the way. So when it's not connected, another question would be\n",
      "[07:20.400 --> 07:28.880]  how many components does it have? Is there some sort of a nice structure to the components of this graph?\n",
      "[07:30.000 --> 07:35.200]  And then another question would be what is the diameter of the circumfiguration graph or of\n",
      "[07:35.200 --> 07:39.680]  each one of its components? And that's usually a very important question to ask when you're dealing with\n",
      "[07:39.680 --> 07:46.880]  one player games because this could tell you like what would be the worst possible shortest path\n",
      "[07:46.880 --> 07:51.600]  to reach a target configuration or to solve your game, to win your game, for example.\n",
      "[07:52.160 --> 07:58.160]  And in the literature, this is sometimes known as God's number, which would be the diameter of\n",
      "[07:58.160 --> 08:04.400]  the reconfiguration graph. And these are all very interesting, very interesting structural questions\n",
      "[08:04.480 --> 08:11.680]  to ask about this reconfiguration graph. Now on the algorithmic side or the computational side,\n",
      "[08:11.680 --> 08:18.320]  there's the obvious question of if I'm given a starting state and some ending state or target state,\n",
      "[08:18.320 --> 08:23.520]  like in the case of the puzzle game, that I am given some starting state and we know what the\n",
      "[08:23.520 --> 08:29.520]  goal state is. So here one decision problem would be to answer the question whether it's possible to\n",
      "[08:29.600 --> 08:34.320]  get to the target state starting from some initial state that is also given to me.\n",
      "[08:35.840 --> 08:40.320]  So you can decide to solve this problem either as a decision problem or as a search problem,\n",
      "[08:40.320 --> 08:45.920]  which would give you the actual sequence of steps that will take you from a state to the target state.\n",
      "[08:49.760 --> 08:55.520]  Other interesting computational problems is it always possible to go from one configuration to\n",
      "[08:55.600 --> 09:01.200]  any other and this is basically also related to the structural question about connected components.\n",
      "[09:02.400 --> 09:09.120]  And the last question that I will mention, which is also interesting, is how fast can you go\n",
      "[09:09.120 --> 09:13.680]  from one configuration to another? Meaning can you do it in at most case steps?\n",
      "[09:14.560 --> 09:20.400]  There is a question I should wait or no.\n",
      "[09:25.280 --> 09:33.520]  All right. So think about all of these questions that we paused using the simple 15 puzzle game.\n",
      "[09:34.080 --> 09:40.160]  And now we're going to look at a lot of other possible problems where the same\n",
      "[09:40.160 --> 09:45.760]  any configuration graph can be extracted. And we can ask the same set of questions.\n",
      "[09:46.800 --> 09:52.400]  So all of you here are familiar with the case sad problem. So you're given a Boolean formula and\n",
      "[09:52.400 --> 09:58.960]  you want to know if you can satisfy this formula by assigning values to the variables. And we know\n",
      "[09:58.960 --> 10:04.880]  that this is NP complete for K greater than or equal to three. So now how can you transform this\n",
      "[10:04.880 --> 10:10.080]  into a reconfiguration problem? Well, it's very simple. So now you're given a formula\n",
      "[10:11.440 --> 10:16.720]  and you're given two satisfying assignments. So you can think of those satisfying assignments as\n",
      "[10:16.720 --> 10:24.480]  bit vectors. And so now the question that you can ask is can I go from the first satisfying assignment\n",
      "[10:24.480 --> 10:32.800]  as to the next one? By basically flipping one bit at a time, under the condition that I remain\n",
      "[10:32.800 --> 10:39.200]  a satisfying assignment at all times. And notice that without this condition, the problem is trivial.\n",
      "[10:40.880 --> 10:48.080]  So you can basically just flip the bits however you like and reach S from T or T from S. But once\n",
      "[10:48.080 --> 10:54.400]  you add this constraint of you should remain a satisfying assignment, the problem becomes way\n",
      "[10:54.400 --> 11:01.520]  more interesting. And you can think of this problem again as walking in the solution space of the\n",
      "[11:01.600 --> 11:11.520]  given formula of all the satisfying assignment of the formula. All right, so that's the sad reconfiguration\n",
      "[11:11.520 --> 11:20.400]  problem. Let's look at another example. Graph coloring. We all know it. We all love it. You're\n",
      "[11:20.400 --> 11:26.320]  given a graph and some integer K and you are asked whether you can properly take color the\n",
      "[11:26.320 --> 11:30.560]  graph G. And we know again that this is NP complete for K greater than or equal to three.\n",
      "[11:31.120 --> 11:36.800]  How do you transform that into a reconfiguration problem? Well, now you're given a graph,\n",
      "[11:36.800 --> 11:44.000]  you're given two colorings of the graph, alpha and beta. And the question is can you recolor alpha\n",
      "[11:44.000 --> 11:51.040]  to get the to beta? But you need to recolor one vertex at a time and you need to remain a proper\n",
      "[11:51.040 --> 11:58.640]  K coloring throughout. Same idea again leads us to this notion of the reconfiguration space\n",
      "[11:58.640 --> 12:03.360]  where we are looking at the K colorings of the graph and how they are connected\n",
      "[12:04.320 --> 12:09.200]  under this adjacent simulation that we define, which is a single vertex recoloring.\n",
      "[12:11.920 --> 12:17.280]  The final example that I will mention, which will be basically what we will focus on in the rest of\n",
      "[12:17.280 --> 12:23.440]  the talk is token placement. I call it, but as you will all guess, this is the famous independent\n",
      "[12:23.440 --> 12:29.360]  set problem. But we will look at it as a token placement problem because it will be more useful\n",
      "[12:29.360 --> 12:34.080]  for the rest of the talk. So you're given a graph G and an integer K. And the question is,\n",
      "[12:34.080 --> 12:40.640]  can you place K tokens on your graph K black tokens so that no two of these tokens share an edge.\n",
      "[12:40.640 --> 12:46.640]  And of course, we all know that this is an NP complete problem. So how can you transform this\n",
      "[12:46.640 --> 12:52.480]  problem into a reconfiguration problem? Again, now I'm giving a graph two independent sets of the graph.\n",
      "[12:54.160 --> 12:58.800]  Each of size K. And the question is, can I go from one independent set to the other\n",
      "[13:00.320 --> 13:07.440]  under what rule? So here defining the rule for independent set, how can I go between consecutive\n",
      "[13:07.440 --> 13:14.160]  independent sets becomes a little bit less obvious. And there are two main strategies that people\n",
      "[13:14.160 --> 13:21.040]  have attempted. So the first rule is what we call token jumping. So you are basically allowed to\n",
      "[13:21.040 --> 13:27.760]  take any token on your graph and jump it to any other vertex on the graph, assuming that it doesn't\n",
      "[13:27.760 --> 13:35.760]  have a token and that you maintain an independent set at all times. So for example, in this example that\n",
      "[13:35.760 --> 13:41.840]  I have here, it would be perfectly okay to take this token here and jump it to this vertex here.\n",
      "[13:41.840 --> 13:52.080]  Or I could also take this token here and jump it to this vertex here. So that, no, actually that\n",
      "[13:52.080 --> 13:59.280]  would violate the independence. So you can jump to any other vertex as long as you maintain independence.\n",
      "[13:59.280 --> 14:06.720]  And we call that the token jumping rule. The other rule is basically token sliding. So in this case,\n",
      "[14:06.720 --> 14:15.920]  we only allow a token to slide along edges of the graph. So a token can only move to adjacent\n",
      "[14:15.920 --> 14:22.560]  vertex, assuming of course, this does not violate independence. So now we have two different\n",
      "[14:22.560 --> 14:27.360]  reconfiguration graphs we can think about. We can think about the reconfiguration graph under the\n",
      "[14:27.360 --> 14:32.800]  token jumping adjacency. And we can think about the reconfiguration graph under the token sliding\n",
      "[14:32.800 --> 14:39.120]  adjacency. And we're going to talk about these two different problems because they do actually\n",
      "[14:39.120 --> 14:46.000]  behave quite differently and they produce quite interesting results like the difference between the two.\n",
      "[14:46.000 --> 14:51.840]  We don't fully understand yet, but we kind of know that token sliding can be harder than token\n",
      "[14:51.840 --> 15:00.560]  jumping. But there's still a lot of questions to be answered. All right. So some of you might be asking,\n",
      "[15:00.560 --> 15:08.480]  why do we care about studying such problems? There's a lot of motivations out there. I mean,\n",
      "[15:09.600 --> 15:15.120]  sometimes I would say you don't need motivation. They're interesting. There's a lot of open\n",
      "[15:15.120 --> 15:20.720]  questions that we need to answer. But you can also think about reconfiguration problems as another\n",
      "[15:20.720 --> 15:26.560]  way of modeling real world algorithmic problems because you usually never start from scratch.\n",
      "[15:27.120 --> 15:32.080]  When you're trying to solve real world problems, you usually start from something and you're trying\n",
      "[15:32.080 --> 15:37.760]  to prove it or make it better or change it to something more appropriate. Another very good\n",
      "[15:38.640 --> 15:44.720]  application of studying these problems is that they give you a better understanding of solution\n",
      "[15:44.720 --> 15:50.960]  spaces, which can be very important for other areas as well. And they have been used in statistical\n",
      "[15:50.960 --> 15:57.520]  physics, quantum computing, any complexity theory, combinatorics, and robotics, and hopefully many more\n",
      "[15:57.520 --> 16:03.200]  applications to come. But what I would tell you is that there are so many very interesting problems\n",
      "[16:03.200 --> 16:08.800]  that are so easy to start thinking about without having too much background, which is what I think\n",
      "[16:08.800 --> 16:15.120]  this is a very nice area to start working on at any level in your research career.\n",
      "[16:15.200 --> 16:25.200]  All right, so I'll take a break here and take questions if there are any. And then we will dive into\n",
      "[16:25.200 --> 16:32.640]  the token jumping and token sliding problems, what we know about them in terms of classical complexity,\n",
      "[16:32.640 --> 16:37.280]  and what was basically the starting point for the project that led us to this paper.\n",
      "[16:37.760 --> 16:41.920]  Any questions at this point?\n",
      "[16:45.200 --> 16:51.600]  I'm, I apologize for the small context, which I am in interrupting here. So this is just to\n",
      "[16:51.600 --> 16:57.680]  announce for the PC301 workshop that will be happening in December end. And this will be slightly\n",
      "[16:57.680 --> 17:03.120]  different from the previous two workshops. First major difference, this will be online. Second is\n",
      "[17:03.760 --> 17:11.840]  some advanced topics, what we discuss. So anyone who intends to explore somewhat more complex\n",
      "[17:11.840 --> 17:18.480]  topics in parametrize algorithms is invited to have a check. They can look at the website that\n",
      "[17:18.480 --> 17:23.200]  has been shared on the chat. And if you wish, you can register simply by filling a form that is\n",
      "[17:24.400 --> 17:30.480]  linked at the bottom of the webpage. So just to inform you all about it. And sorry for the\n",
      "[17:30.480 --> 17:42.000]  introduction for this now. All right. All right. So let's start talking about token jumping,\n",
      "[17:42.000 --> 17:48.080]  token sliding, and a little bit about classical complexity. I know everybody here knows about\n",
      "[17:48.080 --> 17:53.120]  P and NPs. So I'm not going to talk about this. Some of you might not be familiar with the\n",
      "[17:53.120 --> 17:59.520]  P space class. So just a quick note that's as much as you will need to know for this talk is that\n",
      "[17:59.520 --> 18:05.840]  P space is the set of all decision problems that can be solved using a polynomial amount of space.\n",
      "[18:06.720 --> 18:13.360]  And the reason why I mentioned this class is because many many many many the configuration problem\n",
      "[18:13.360 --> 18:20.400]  actually are P space complete. Okay. And so so what we know the standard inclusion is we know that\n",
      "[18:20.400 --> 18:26.480]  P is contained in NP, which is contained in P space. But a very useful thing about P space is\n",
      "[18:26.480 --> 18:34.080]  that savage prove that it's equal to NP space. So polynomial space and non deterministic polynomial\n",
      "[18:34.080 --> 18:41.040]  space are the same class. Basically, and that's extremely useful when you start to think about\n",
      "[18:41.040 --> 18:45.760]  reconfiguration problems because if you think about reconfiguration problem where you're given some\n",
      "[18:45.760 --> 18:53.520]  state and you want to reach the other one. So basically you can solve that easily and non deterministic\n",
      "[18:53.520 --> 19:02.400]  polynomial space, which basically implies that they are NP space. But actually you can show a lot\n",
      "[19:02.400 --> 19:08.160]  more than that. You can show that many really many reconfiguration problems are actually P space\n",
      "[19:08.160 --> 19:13.760]  complete, which is not surprising. Right. The fact that many of these reconfiguration problems are\n",
      "[19:13.760 --> 19:22.640]  P space complete is not very surprising. Right. And then not being in NP is because they don't always\n",
      "[19:22.640 --> 19:28.560]  have polynomial size certificates, which also makes sense because sometimes the number of steps\n",
      "[19:28.560 --> 19:34.080]  that you need to take to go from one configuration to the other might very well be exponential in the\n",
      "[19:34.080 --> 19:41.120]  graph. But there are also some extremely surprising results. And these are some of the results some of\n",
      "[19:41.120 --> 19:48.640]  my favorite results in the area. So for example, you all know that coloring is NP complete even for\n",
      "[19:48.880 --> 19:56.320]  K equals three. However, it turns out that if you try to solve the recoloring problem for K equals\n",
      "[19:56.320 --> 20:02.640]  three, it's actually polynomial time solvable. So if I give you two three colorings of a graph and\n",
      "[20:02.640 --> 20:09.680]  I ask you, is there a path between them that recolors one vertex at a time and never is and is\n",
      "[20:09.680 --> 20:15.840]  always a valid three coloring, then this problem can be solved in polynomial time. And the recoloring\n",
      "[20:15.840 --> 20:23.600]  problem only becomes P space complete for K equals four and more. Right. So that's the first\n",
      "[20:24.480 --> 20:31.760]  surprising result. Another very surprising result is that as as your all FBT experts here, I know\n",
      "[20:31.760 --> 20:37.440]  that you're all familiar with the fact that usually when we study problems on graphs of bounded bucket\n",
      "[20:37.760 --> 20:46.080]  path with tree width, they tend to become easier. It turns out that that's not really the case\n",
      "[20:46.800 --> 20:51.360]  for reconfiguration problems, at least for token sliding and jumping, which is the two problems\n",
      "[20:51.360 --> 20:57.280]  that are related to independent set. It turns out that those two problems remain P space complete\n",
      "[20:57.280 --> 21:03.440]  even if you have a graph of constant tree width or path width or even bucket width. So a very,\n",
      "[21:03.440 --> 21:08.640]  very, very simple graph structure still the problem remains hard.\n",
      "[21:11.120 --> 21:18.560]  All right. And finally, the last theorem that I also like a lot shows you basically that\n",
      "[21:18.560 --> 21:26.160]  sliding and jumping behave differently. And it was shown that if you restrict yourself to\n",
      "[21:26.160 --> 21:31.440]  bipartite graphs, where we know that max and independent set can be solved in polynomial time,\n",
      "[21:32.240 --> 21:38.560]  if you restrict yourself to those graphs, it turns out that token jumping is NP complete,\n",
      "[21:39.760 --> 21:46.400]  whereas token sliding is P space complete, which is a which is a strange\n",
      "[21:48.480 --> 21:50.880]  difference between the behavior of those two problems.\n",
      "[21:54.240 --> 21:54.640]  All right.\n",
      "[21:55.120 --> 22:04.080]  So in fact, we know a lot more about token sliding and token jumping. These problems have been\n",
      "[22:04.080 --> 22:08.960]  at the heart of the area of combinatorial reconfiguration. They have been studied so much.\n",
      "[22:09.920 --> 22:15.120]  And we know so much about them at least in terms of standard or classical complexity.\n",
      "[22:15.680 --> 22:22.160]  So some of the important results for our paper that we're going to focus on\n",
      "[22:22.880 --> 22:30.080]  is this result. So that's going to be the starting point of the results that we will discuss\n",
      "[22:30.080 --> 22:35.040]  next when we move to parametrize complexity. So the fact that token sliding and token jumping\n",
      "[22:36.640 --> 22:41.520]  are b space complete and then NP complete respectively on bipartite graphs was the starting\n",
      "[22:41.520 --> 22:46.320]  point of our next paper. But there are some very interesting results here that are also worth\n",
      "[22:46.320 --> 22:51.600]  mentioning. So for example, for even whole figure halves, we know how to solve token jumping\n",
      "[22:51.600 --> 22:58.240]  in polynomial time. But the complexity of independent set even remains open on this class of graphs.\n",
      "[22:59.600 --> 23:05.360]  And the complexity of token sliding also remains open. So we don't know how to check if given\n",
      "[23:05.360 --> 23:11.440]  two independent sets, I can slide one to the other. Can you answer that question in polynomial\n",
      "[23:11.440 --> 23:18.880]  time for even whole free graphs? For split graphs and chordal graphs, they also behave\n",
      "[23:18.880 --> 23:24.320]  extremely differently token sliding and token jumping. So they are token sliding is p space\n",
      "[23:24.320 --> 23:29.600]  complete on split graphs and chordal graphs while token jumping is polynomial time.\n",
      "[23:30.800 --> 23:38.160]  And that is some of the reasons why we feel that token sliding is harder usually than token jumping.\n",
      "[23:38.160 --> 23:48.480]  But it's not always the case. All right. So that's it for classical complexity.\n",
      "[23:49.840 --> 23:55.920]  So now let's move on to parameterized complexity. And let's basically think about how you can\n",
      "[23:55.920 --> 24:04.080]  parameterize those two problems, token jumping and token sliding. So the obvious parameter would be\n",
      "[24:04.080 --> 24:09.280]  the number of tokens. So one of the obvious parameters would be the number of tokens. So\n",
      "[24:10.320 --> 24:15.440]  and we're going to denote that by K. Another parameter would be the length of the sequence.\n",
      "[24:15.440 --> 24:20.000]  Like how many steps does it take to go from one independent set to the other?\n",
      "[24:20.880 --> 24:25.600]  You can also obviously parameterize by tree width or path width or any combination of the above.\n",
      "[24:26.880 --> 24:34.080]  When we started working on this problem, our initial aim was to basically study the\n",
      "[24:34.080 --> 24:40.160]  parameterized complexity of token sliding and token jumping on bipartite graphs using the parameter\n",
      "[24:40.160 --> 24:46.320]  K number of tokens. Right? Because remember, we saw that token sliding is piece-based\n",
      "[24:46.320 --> 24:53.120]  complete on bipartite graphs and token jumping is NP. So you were interested to see if basically\n",
      "[24:53.120 --> 24:59.120]  this is going to give us W1 hardness for token sliding and FPTNES for token jumping.\n",
      "[25:00.800 --> 25:05.440]  All right. At least that was the initial hope. That's why we started working on this project.\n",
      "[25:06.160 --> 25:12.000]  We weren't able to answer the two questions. So we were able to answer one side of the question,\n",
      "[25:12.720 --> 25:20.720]  which is we were able to show that on bipartite graphs, token sliding is in fact W1 hardness.\n",
      "[25:22.320 --> 25:28.160]  So token sliding parameterized by the number of tokens on bipartite graphs is W1 hard.\n",
      "[25:28.720 --> 25:35.120]  We were not able to answer the question for token jumping. So that is still an open question.\n",
      "[25:36.000 --> 25:42.800]  So having answered that question and failed on the next question, we started thinking about ways\n",
      "[25:42.800 --> 25:49.760]  to basically simplify a little bit some of these questions. So the next thing we asked ourselves,\n",
      "[25:49.760 --> 25:56.320]  so there are two directions where you can try and simplify. So the next thing we asked\n",
      "[25:56.320 --> 26:02.480]  ourselves was, okay, so from bipartite graphs, how can I go to other classes of graphs\n",
      "[26:03.200 --> 26:11.200]  and see where token jumping becomes hard or easy? And it turned out that if you basically exclude\n",
      "[26:11.200 --> 26:21.120]  only C4 from your graph, right? And so we, because in bipartite graphs, you're excluding all odd cycles.\n",
      "[26:22.640 --> 26:28.160]  Right. So, and we started thinking about what kinds of cycles affect the behavior of those\n",
      "[26:28.160 --> 26:33.520]  problems? So the first question was, what about C4 free graphs? And it turned out that both\n",
      "[26:33.520 --> 26:42.720]  problems remained W1 hard on C4 free graphs. Now, if you exclude C3 and C4, it turns out that\n",
      "[26:42.720 --> 26:49.920]  token jumping becomes FPD has an order K squared kernel. But for token sliding, we were not able\n",
      "[26:50.560 --> 26:57.760]  to determine the complexity. Now, if you go to the other side of that,\n",
      "[26:58.400 --> 27:07.280]  so what if we enforce both bipartiteness as well as C4 freeness? So in that case, we were able to\n",
      "[27:07.280 --> 27:16.880]  show that both problems became FPD. Okay. And basically, the bipartite bounded degree graphs was\n",
      "[27:16.880 --> 27:25.200]  just a stepping stone to get to the bipartite C4 free graph result. So let me, let me repeat that\n",
      "[27:25.200 --> 27:30.880]  maybe slightly more clearly. So after basically answering the first question, which was bipartite\n",
      "[27:30.880 --> 27:36.160]  graphs, we were able to show that token sliding was W1 hard, but we were not able to determine\n",
      "[27:36.160 --> 27:43.120]  the complexity of token jumping. So then we went to C4 free graphs and we were able to show that both\n",
      "[27:43.120 --> 27:50.640]  problems are actually W1 hard. Then if we added one more constraint, which was C3 C4 free graphs,\n",
      "[27:50.640 --> 27:55.280]  we got FPDness for token jumping, but it remained open for token sliding.\n",
      "[27:56.400 --> 28:02.960]  And on the other side of the spectrum, so if we keep bipartite and enforce the C4 freeness,\n",
      "[28:02.960 --> 28:13.760]  we get FPD for both problems. And as a side note, this blue result is not part of our paper. This\n",
      "[28:13.760 --> 28:22.880]  was known prior to our paper. So any questions about the results?\n",
      "[28:22.880 --> 28:49.200]  No questions. All right. Cool. So lots of open problems. The first and obvious one is,\n",
      "[28:49.840 --> 28:54.640]  what is the pattern is token jumping FPD, parameterized by K on bipartite graphs.\n",
      "[28:55.280 --> 29:01.360]  And that's really, I mean, that was the initial question that we set out to answer and couldn't.\n",
      "[29:01.360 --> 29:12.640]  So that remains open. And it's, so I will not be going over the hardness reduction for token sliding\n",
      "[29:12.640 --> 29:18.320]  on bipartite graphs because it's quite technical. I don't feel a talk is the right place to go over it.\n",
      "[29:19.520 --> 29:27.120]  But if you go over the reduction, you will see that that it's the two problems really behave\n",
      "[29:27.120 --> 29:32.240]  differently. And there that doesn't seem to be a chance to basically make the same type of\n",
      "[29:32.240 --> 29:39.920]  reduction work for token jumping. So the second interesting open question is, how about token jumping\n",
      "[29:39.920 --> 29:45.920]  parameterized by K on triangle free graphs? That's basically even more general than question one.\n",
      "[29:46.880 --> 29:53.360]  Right. So, and the reason why I mentioned this question separately is because almost every reduction\n",
      "[29:53.360 --> 30:01.280]  that I know of includes large clicks. So you need to use large clicks in your reductions. So how about\n",
      "[30:01.280 --> 30:08.080]  if we don't allow triangle and large clicks? So can we can we can we then say something about the problem?\n",
      "[30:08.960 --> 30:16.320]  So that's for token jumping. Now, when it when you go to token sliding. So, so the open problem is\n",
      "[30:17.280 --> 30:23.520]  what happens for token sliding on graphs of girth at least five, so if they are C3, C4 free.\n",
      "[30:24.240 --> 30:31.440]  Or you can even make that a bit weaker and ask for any girth of at least P for some constant P.\n",
      "[30:33.840 --> 30:37.200]  And for all of these questions, of course, polynomial kernels.\n",
      "[30:38.560 --> 30:44.800]  Would be interesting as well because in our case, we do get polynomial kernels for the FB.\n",
      "[30:47.600 --> 30:53.360]  And the polynomials are not great, but polynomial regardless.\n",
      "[30:56.640 --> 31:03.680]  All right. So, in the rest of the talk, I will try to cover some of the technical stuff.\n",
      "[31:03.680 --> 31:09.360]  And as promised, I will try to keep it as light as possible so that I can give you some of a lot\n",
      "[31:09.360 --> 31:15.920]  of the intuition and techniques that are used in this paper and that are generally used when\n",
      "[31:15.920 --> 31:22.160]  dealing with reconfiguration problems. So the first result that we will go over is this W hardness\n",
      "[31:22.160 --> 31:28.560]  on C4 free graphs. Right. For both token sliding and token jumping. It's the same reduction and\n",
      "[31:29.520 --> 31:35.600]  you will get both results because we will be using maximum independent sets.\n",
      "[31:36.480 --> 31:43.120]  So if you're trying to basically do token sliding from one maximum independent set to the other,\n",
      "[31:43.760 --> 31:48.880]  or token jumping, these two rules become equivalent. Jumping becomes equivalent to sliding.\n",
      "[31:49.520 --> 31:54.400]  So when you're dealing with maximum independent sets, these two basically rules are the same.\n",
      "[31:55.360 --> 31:59.440]  And that's what we're going to do. But what we're going to prove actually is a stronger\n",
      "[31:59.440 --> 32:05.200]  theorem. What we're going to prove is the following theorem. If you take any p greater than or\n",
      "[32:05.200 --> 32:15.200]  equal to four, then both problems are W hard on C4, C5 dot dot dot up to CP free graphs,\n",
      "[32:15.920 --> 32:27.120]  which implies of course C4 free graphs. But you can basically exclude any cycles from C4 up to CP\n",
      "[32:27.120 --> 32:30.640]  for constant P and the problems will remain W1 hard.\n",
      "[32:36.960 --> 32:43.520]  So how do we prove this result? In fact, we use a known reduction from a problem known as grid\n",
      "[32:43.520 --> 32:51.440]  tiling, which is a W1 hard problem. And grid tiling is reduced to the independent set problem\n",
      "[32:51.440 --> 33:00.320]  on C4 up to CP free graphs. And that reduction was used to show that independent set remains\n",
      "[33:00.320 --> 33:09.360]  W1 hard if you exclude C4 up to CP for any constant P. But what is interesting and useful in that\n",
      "[33:09.360 --> 33:16.160]  reduction is the graph that is obtained from the reduction. So the graph that is obtained from\n",
      "[33:16.160 --> 33:22.640]  the reduction has three properties that are going to be useful to us. The first property is that\n",
      "[33:22.640 --> 33:31.680]  you can partition the graph into basically 8k squared into P plus 1 clicks. So you have a bunch of\n",
      "[33:31.680 --> 33:36.880]  clicks each of size n and all of the edges basically are between the clicks.\n",
      "[33:38.960 --> 33:44.560]  But that's it, that's it, that's the whole of the graph. It's a bunch of clicks and edges between them.\n",
      "[33:45.680 --> 33:52.480]  Of course, the more important property as well here is that this graph is going to be C4 up to CP free.\n",
      "[33:52.880 --> 34:03.040]  It will not have any of those cycles as an induced sub graph. And it's an equivalent instance to\n",
      "[34:03.040 --> 34:11.200]  the grid tiling. And that basically gives you a W1 hardness of independent set on this class of graphs.\n",
      "[34:13.360 --> 34:19.600]  So notice in this case that an independent set of size 8k squared into P plus 1 will have to\n",
      "[34:19.600 --> 34:23.760]  be a maximum independent set because that's how many clicks we get in the resulting graph.\n",
      "[34:24.480 --> 34:29.920]  And that's basically the sizes that we will be working with, more or less up to some modifications.\n",
      "[34:30.400 --> 34:38.080]  But this will allow us to basically conclude that both sliding and jumping are hard on this class of\n",
      "[34:38.080 --> 34:48.480]  graphs. So how do we use this for showing hardness of token sliding and token jumping? And let's\n",
      "[34:48.480 --> 34:54.480]  focus on token sliding for now because it's going to be the same anyway. So we have those clicks\n",
      "[34:55.040 --> 35:02.080]  and some edges that go between the clicks. So the first attempt would be as follows. We will add\n",
      "[35:02.800 --> 35:08.640]  a universal vertex to each one of the clicks and we will call this the starting set or the starting\n",
      "[35:08.640 --> 35:14.560]  independent set. And then we add another universal vertex to each one of the clicks and call this\n",
      "[35:14.640 --> 35:20.400]  the target independent set. And now basically we have our instance of token sliding. We want to\n",
      "[35:20.400 --> 35:30.160]  slide everybody in S down to T. So notice that this is useful because we don't introduce any of the\n",
      "[35:30.160 --> 35:37.680]  forbidden cycles. So we are still fine. And if we could guarantee that all of the tokens\n",
      "[35:37.680 --> 35:44.480]  will be on the on the clicks simultaneously, then this will imply an independent set in the\n",
      "[35:44.480 --> 35:50.800]  original graph, which concludes our proof. But unfortunately in this case, we definitely cannot\n",
      "[35:50.800 --> 35:58.640]  conclude that because each rent token can slide independently here and then here and then the\n",
      "[35:58.640 --> 36:05.040]  next one can follow, etc, etc, etc. So you need some way of forbidden, are forbidding\n",
      "[36:06.000 --> 36:14.400]  the tokens to behave freely. We want to make sure that they will all be inside the clicks simultaneously\n",
      "[36:14.400 --> 36:19.760]  and we will be done. And notice that we're going to have 8k squared and 2p plus 1 tokens, right?\n",
      "[36:19.760 --> 36:27.200]  1 for each click and 2 universal vertices for each click. So how do we fix this time of 1080\n",
      "[36:27.760 --> 36:35.760]  issue? Well, here's how we can do it. So instead of simply adding universal vertices,\n",
      "[36:36.560 --> 36:42.240]  we're also going to add an edge between every two universal vertices of a click. And then we're\n",
      "[36:42.240 --> 36:49.600]  going to add something that we call a switch. And in this case, it's a simple edge. And the red token\n",
      "[36:50.160 --> 36:57.440]  needs to go to the blue position. Right? So now we have one extra token inside our graph.\n",
      "[36:57.440 --> 37:08.400]  But now notice what happens. If any red token wants to come to the blue position, then this red token\n",
      "[37:08.400 --> 37:14.720]  needs to be moved to this position before. And if you move that token up to the blue position,\n",
      "[37:14.800 --> 37:20.320]  then you can no longer have any of the red tokens on the universal vertices, which means that they\n",
      "[37:20.320 --> 37:27.360]  will all have to be simultaneously inside the clicks. And now we get the behavior that we want.\n",
      "[37:29.360 --> 37:34.560]  So now we can guarantee that if there is a sequence that takes the red tokens to the blue position,\n",
      "[37:36.000 --> 37:43.040]  then somewhere along that sequence, the tokens are all going to be within the clicks. Unfortunately,\n",
      "[37:43.040 --> 37:48.720]  what happened here is we might have introduced some of the forbidden cycles. We can no longer\n",
      "[37:48.720 --> 37:56.960]  guarantee that this is C4 up to CP3. So what you can do in this case to solve this problem,\n",
      "[37:56.960 --> 38:02.720]  and I'm not going to go into the details, but the intuition should be pretty clear, is that you can\n",
      "[38:02.720 --> 38:09.280]  subdivide those edges, make them long enough so that you don't introduce any forbidden cycles,\n",
      "[38:09.360 --> 38:13.040]  and add appropriate tokens inside of them to get the same behavior.\n",
      "[38:15.200 --> 38:19.760]  Because notice that the number of such edges is bounded by a function of K,\n",
      "[38:20.640 --> 38:24.400]  by a function of yes K and P. This case.\n",
      "[38:26.240 --> 38:33.040]  Right, so you can make these edges subdivide them as many times as needed, add as many tokens\n",
      "[38:33.040 --> 38:37.920]  as needed to maintain all the properties that we need, and to maintain that we're going from one\n",
      "[38:37.920 --> 38:44.720]  maximum independent set to the other, which will give you W1 hardness for both token sliding as well\n",
      "[38:44.720 --> 38:55.200]  as token jumping. All right, questions?\n",
      "[38:55.360 --> 39:10.720]  No questions? All right, so let's keep going.\n",
      "[39:13.680 --> 39:17.600]  So now I'm going to talk about some positive result.\n",
      "[39:18.000 --> 39:23.040]  So the result that I'm going to talk about is this one here.\n",
      "[39:23.920 --> 39:33.120]  Right, so I'm going to show you that on C3, C4 free graphs, token jumping is actually FPD and has\n",
      "[39:33.120 --> 39:38.400]  a quadratic kernel, but again, what we will show is a stronger result.\n",
      "[39:39.360 --> 39:48.880]  So what we will show is the following theorem. What we will show is can be summarized as follows.\n",
      "[39:48.880 --> 39:55.440]  So if you look at any graph or at any instance of the token jumping problem.\n",
      "[39:55.440 --> 40:00.800]  So remember, an instance of token jumping has the input graph, the starting set, the target set,\n",
      "[40:00.800 --> 40:09.920]  and K as the number of tokens. So let me try and draw something here. So if you look at\n",
      "[40:11.040 --> 40:16.160]  your graph, you can kind of decompose it into something which is more or less as follows.\n",
      "[40:17.120 --> 40:21.760]  So you have S, you have T, the intersection need not be empty,\n",
      "[40:23.280 --> 40:26.640]  and then you have the neighborhood of S union T,\n",
      "[40:26.800 --> 40:30.480]  and then you have the rest of the graph.\n",
      "[40:33.040 --> 40:39.360]  So we're going to call the rest of the graph H, and we're going to call the close neighborhood\n",
      "[40:39.360 --> 40:46.320]  of S union D, or if you will, this yellow part here, we call that J.\n",
      "[40:47.120 --> 40:52.080]  Right, so we can think of our problem of our graph as being decomposed into those two areas.\n",
      "[40:52.640 --> 40:58.560]  H and J. Okay, so the theorem states the following.\n",
      "[40:59.520 --> 41:06.480]  If H is epsilon sparse, where epsilon sparse means that the number of edges is at most\n",
      "[41:06.480 --> 41:15.680]  n squared minus epsilon, positive epsilon. So if H is epsilon sparse, and J is C3C for free,\n",
      "[41:16.480 --> 41:24.400]  then the problem admits a kernel, which is that big, K squared plus K into 1 plus 1 over epsilon.\n",
      "[41:25.280 --> 41:34.640]  So notice now that we only need that H is epsilon sparse, and we only require C3C for\n",
      "[41:34.720 --> 41:41.360]  freeness inside J, which is S union T close neighborhood of S union T.\n",
      "[41:45.120 --> 41:50.320]  And this idea is actually is not a new idea. So this idea is,\n",
      "[41:51.600 --> 41:55.920]  okay, I had the drawing here, I should have used it. So the idea comes from,\n",
      "[41:55.920 --> 42:02.160]  from has been used before, and it's what we call the buffer technique for the token jumping problem.\n",
      "[42:02.240 --> 42:08.880]  And the intuition behind the buffer technique is very simple. So if I have S union T, but somewhere\n",
      "[42:09.760 --> 42:15.120]  in the graph, which is not in the close neighborhood of S union T, I have a case sized\n",
      "[42:15.120 --> 42:21.360]  independent set, then you are done. Right, if I have a case sized independent set in H,\n",
      "[42:22.880 --> 42:28.720]  then you're done. You can basically take all the tokens on S, jump them into those\n",
      "[42:29.360 --> 42:32.960]  independent yellow vertices in H, and then jump them back to T.\n",
      "[42:34.480 --> 42:39.120]  So in some sense, when H has a large independent set, that's the easy case.\n",
      "[42:40.880 --> 42:44.960]  Right, you're done. If you can find a large enough independent set in H, you're done.\n",
      "[42:46.240 --> 42:50.960]  And that's what we call the buffer technique, because it's been also used to show that the problem\n",
      "[42:50.960 --> 42:58.160]  is FBT on planar graphs, for example, or K3J free graphs. So graphs without large bi-clips.\n",
      "[42:59.280 --> 43:12.720]  So it's a well-known technique. All right. So what do we show? So we're going to use the buffer technique,\n",
      "[43:12.720 --> 43:20.480]  and we're going to combine it with something else. So we show that you have a yes instance\n",
      "[43:21.440 --> 43:29.760]  whenever one of those two conditions is true. The first condition is that H is epsilon sparse and\n",
      "[43:29.760 --> 43:38.320]  contains more than this many vertices. And this is relatively easy. When you contain this\n",
      "[43:38.320 --> 43:43.680]  many vertices and you add epsilon sparse, then you will have a case size independent set.\n",
      "[43:43.680 --> 43:49.440]  And that's basically the buffer technique. When H is epsilon sparse and has that many vertices\n",
      "[43:49.440 --> 43:54.000]  or more, then H is guaranteed to have an independent set of sparse and you're done.\n",
      "[43:55.520 --> 44:02.000]  So now you are stuck with what happens inside J, or the closed neighborhood of S-Union T.\n",
      "[44:02.560 --> 44:09.920]  And it turns out there, if you have C3C for freeness, the only thing you need on top of that,\n",
      "[44:09.920 --> 44:14.080]  to guarantee a yes instance, is a vertex of degree at least 3K.\n",
      "[44:14.560 --> 44:24.880]  So if you have C3C for freeness inside J, and the vertex of degree 3K, then again you get a yes instance.\n",
      "[44:24.880 --> 44:34.880]  So let me prove those two statements separately, because they will be basically what we need for the\n",
      "[44:34.880 --> 44:44.000]  final theorem for the final theorem. So the first lemma, as I told you, if H is epsilon sparse and\n",
      "[44:44.000 --> 44:49.600]  has more than this many vertices, then it's a yes instance, because you have a case size\n",
      "[44:49.600 --> 44:55.440]  independent set in H. The idea of this proof is simple. It's a counting argument.\n",
      "[44:56.160 --> 45:01.360]  And what you need to do basically first is to show that H must contain a vertex of degree\n",
      "[45:01.360 --> 45:07.920]  less than and over K. And then basically you apply the standard greedy packing algorithm for\n",
      "[45:07.920 --> 45:13.600]  constructing an independent set of size K. And the reason you show that the way you show that H\n",
      "[45:13.600 --> 45:19.920]  has a vertex of degree less than and over K is, again, standard counting argument and the\n",
      "[45:19.920 --> 45:26.480]  hand-shaking lemma. So if the minimum degree in H was at least n over K, then the number of edges\n",
      "[45:26.480 --> 45:34.880]  would be at least n squared over 2K, which will only happen in an epsilon sparse graph when n is\n",
      "[45:34.880 --> 45:43.120]  less than or equal 2K to the power 1 over S. And the rest of the proof is basically an induction on K.\n",
      "[45:45.200 --> 45:50.160]  Okay, and so that shows you that when you do have an epsilon sparse graph with more than\n",
      "[45:50.800 --> 46:00.480]  this many vertices, then we have a yes instance. All right, so how about the second part of the claim?\n",
      "[46:00.480 --> 46:07.920]  So now what happens if we have a C3 C4 free J that has a vertex of degree 3K? Well, let's see what\n",
      "[46:07.920 --> 46:16.720]  happens. So if we have a vertex of degree 3K and I'm going to circle it here in yellow. So how can\n",
      "[46:16.720 --> 46:22.960]  the neighborhood of that vertex look? Well, we know that J is C3 free. So the blue edges cannot\n",
      "[46:22.960 --> 46:28.960]  exist, which means that the neighborhood of the yellow vertex is an independent set inside J,\n",
      "[46:30.800 --> 46:37.680]  not in the whole graph. Well, in fact, in the whole well known because we're only talking about J\n",
      "[46:37.680 --> 46:44.960]  as a subgratio. Right, so the blue edges cannot exist because otherwise we will get a C3 inside J.\n",
      "[46:46.960 --> 46:57.600]  All right, so now let's look at the other vertices in S union T. The other, the second observation\n",
      "[46:57.600 --> 47:04.080]  that you need is that any vertex other than the yellow vertex can have at most one neighbor in\n",
      "[47:04.080 --> 47:11.600]  common with the yellow vertex. Because if you do have two neighbors in common, then you will get a C4.\n",
      "[47:11.680 --> 47:21.760]  So now what happens if we have three k vertices in the neighborhood of the yellow vertex? Well,\n",
      "[47:22.880 --> 47:30.320]  at most two k of them can be connected to some vertex in S union T and you will get at least k\n",
      "[47:30.320 --> 47:40.240]  of them, some k of them here that are only connected to the yellow vertex. And so now basically,\n",
      "[47:40.240 --> 47:44.720]  instead of using a buffer inside H, we have just found a buffer inside J.\n",
      "[47:46.480 --> 47:52.640]  And we can use the same strategy. We can jump all the tokens here, starting of course by the yellow\n",
      "[47:52.640 --> 47:56.080]  token and then jump them to where they need to go.\n",
      "[47:56.160 --> 48:13.440]  So now combining those two observations together, if you will, we get the following theorem.\n",
      "[48:13.440 --> 48:20.320]  So if H is alpha sparse and J is C3, C4, free, then the problem admits a kernel on this\n",
      "[48:20.320 --> 48:26.400]  maneuver to C's. And it's basically a simple application of the previous two lemmas. If we have\n",
      "[48:26.400 --> 48:32.480]  more than this maneuver to C's in H, it's a trivial yes instance. If J has a vertex of\n",
      "[48:32.480 --> 48:38.800]  degree 3k or more, it's trivial yes instance. And now you combine all of this together. We know that\n",
      "[48:38.800 --> 48:45.280]  S union T is of size at most 2k. We know that the neighborhood of S union T is of size at most\n",
      "[48:45.280 --> 48:52.160]  2k times 3k, which is roughly 6k squared. And now we know that the rest of the graph has at most\n",
      "[48:52.160 --> 48:58.160]  that maneuver to C's. So basically, you sum up those numbers and you get this bound.\n",
      "[49:06.480 --> 49:12.240]  All right. So how does this theorem imply the result that I promised you to start with?\n",
      "[49:15.440 --> 49:22.000]  So that token jumping and token sliding admit kernel with order k squared vertices.\n",
      "[49:23.360 --> 49:29.440]  I mean, I mean, it also holds for bipartite C4 free graphs, right? Obviously because they are C3C4 free.\n",
      "[49:30.240 --> 49:37.840]  So how do you get the kernel? Well, we know that J cannot contain more than 6k squared minus 2\n",
      "[49:37.840 --> 49:48.640]  k vertices. And we know from a theorem from another paper that C3 free graphs with k squared\n",
      "[49:48.640 --> 49:54.880]  over log k vertices must have an independent set of size at least k. And now we know that if H\n",
      "[49:54.880 --> 49:58.640]  contains more than this maneuver to C's, then we will get the yes instance as well.\n",
      "[50:00.880 --> 50:06.640]  Right. So it becomes an immediate consequence of the previous theorem. But the previous theorem\n",
      "[50:06.640 --> 50:12.560]  is even more general than this corollary. So this corollary does not really use the full power of this theorem.\n",
      "[50:16.400 --> 50:23.200]  All right. That's it. I think I'm fine. If you have questions, I will take them now.\n",
      "[50:23.200 --> 50:35.120]  So it was 55 minutes, right? For the talk. I did not go under the talk.\n",
      "[50:35.120 --> 50:39.120]  It's fine. We usually allow plus minus 10 minutes. That's all right.\n",
      "[50:42.400 --> 50:45.920]  So I have a question about token sliding. Yes.\n",
      "[50:46.880 --> 50:54.320]  So how crucial what happens if one does not restrict the independent sets during the configuration\n",
      "[50:55.440 --> 51:02.560]  to be not of the same size? Is that is that very critical for the difficulty or the easiness of the problem?\n",
      "[51:03.680 --> 51:08.560]  Well, you have to be careful how you define that because in token sliding,\n",
      "[51:09.520 --> 51:15.680]  tokens cannot leave the graph. That's correct. But the independent set sequence\n",
      "[51:15.680 --> 51:18.320]  all the independent sets have to be the same size, right?\n",
      "[51:18.320 --> 51:24.080]  Or if not some token disappeared at some point and I'm not sure how it disappeared.\n",
      "[51:26.400 --> 51:31.680]  Right. Because you start with something of size K and you're going to something of size K,\n",
      "[51:31.680 --> 51:40.080]  you cannot leave the graph unless you define it in some way. So you will remain of size K throughout.\n",
      "[51:41.040 --> 51:46.080]  But you can become slightly larger in K. But where does the new token come from?\n",
      "[51:49.040 --> 51:55.120]  So there is a third rule that I did not tell you about which is called token addition and remove.\n",
      "[51:56.960 --> 52:03.520]  Under that rule, we actually allow you to remove vertices and adversities as long as you remain\n",
      "[52:03.600 --> 52:12.240]  an independent set of size at least K. Does that answer your question?\n",
      "[52:12.240 --> 52:19.200]  Yeah, yeah, yeah, yeah. But in fact, it was shown that it was shown that\n",
      "[52:20.720 --> 52:23.680]  so addition and removal is equivalent to token jumping.\n",
      "[52:25.920 --> 52:30.560]  It doesn't it never makes sense to add more tokens to your graph if you don't need them.\n",
      "[52:34.400 --> 52:37.360]  You're only making your life harder into it and be speaking.\n",
      "[52:41.120 --> 52:43.840]  So the other question that I had is I mean I heard I\n",
      "[52:46.720 --> 52:54.640]  so is it possible to view this whole problem on an exponential size graph where every vertex\n",
      "[52:55.280 --> 52:59.680]  corresponds to a independent set in the original graph.\n",
      "[53:00.640 --> 53:06.640]  And then you have edges between two vertices if there is an edge between two vertices of the\n",
      "[53:06.640 --> 53:13.280]  independent set. And now you are doing a reachability question. Is that a meaningful way to think about\n",
      "[53:13.280 --> 53:20.240]  this? But that's exactly what we're doing. But so the way you define your adjacency, I think. So you\n",
      "[53:20.240 --> 53:25.760]  mean you define you make two independent sets adjacent if one can be reached from the other via a\n",
      "[53:25.760 --> 53:30.480]  single slide or a single joint. Exactly. Yeah, one edge here. There is one pair you and me,\n",
      "[53:30.480 --> 53:35.440]  which is adjacent. But that's but that's exactly what we're doing. Okay, okay. Yeah.\n",
      "[53:36.000 --> 53:42.560]  Right? I mean if you because we're looking at algorithms here, we kind of forget the structural\n",
      "[53:42.560 --> 53:47.520]  picture behind it. But this algorithm is finding a path in this graph that you're describing.\n",
      "[53:48.720 --> 53:54.800]  Yeah, yeah, that's it. And what we're saying is you can do it in FBT time or not depending on the\n",
      "[53:54.800 --> 54:13.360]  problem we're talking about. Hi Amir. Hi Amir. Hi. Hi. Hi. Yeah. I'm good. So I had a question. So\n",
      "[54:14.160 --> 54:21.280]  do problems remain equally hard if we bound the if we have a restriction on the number of times,\n",
      "[54:21.280 --> 54:30.720]  we can move the token to a particular vertex. The number of times you can move a token to a\n",
      "[54:30.720 --> 54:36.400]  particular vertex. Like, there's a lot of times the tokens can be moved to a vertex.\n",
      "[54:38.400 --> 54:45.040]  Well, that's definitely going to change the complexity in at least intuitively speaking, right? Because\n",
      "[54:46.000 --> 54:51.040]  now you're saying maybe it will, if you're bounding that by a constant, then you might be saying that\n",
      "[54:51.600 --> 54:58.640]  I'm not allowing exponentially large sequences anymore. But in terms of exactly how the complexity\n",
      "[54:58.640 --> 55:06.480]  changes, I don't have answers. I think it's a very nice question to pose. Even in terms of\n",
      "[55:06.480 --> 55:11.200]  a non-parameterized complexity standard complexity, I think that that would be a very interesting\n",
      "[55:11.200 --> 55:17.360]  question because because it will definitely affect the behavior. I'm not sure exactly how yet.\n",
      "[55:18.320 --> 55:22.640]  I don't know of any results that ask this particular question.\n",
      "[55:23.760 --> 55:30.320]  Okay. So I had one more question in the W. Harnes result that you presented. So do you know\n",
      "[55:30.320 --> 55:38.720]  what is the length of the the length of the changes? Actually, the number of changes or flips that\n",
      "[55:38.720 --> 55:46.880]  you make in your independence? This is just yes. Yes. Yes. Yes. Yes. We do. So here, the number of\n",
      "[55:46.880 --> 55:51.200]  changes is going to be where it's basically going to be the shortest possible sequence.\n",
      "[55:53.040 --> 55:59.840]  So it's going to be, so if you think about the simple construction, this one,\n",
      "[56:04.000 --> 56:09.440]  it's basically literally going to be these guys are going to move here. So H is going to cost me\n",
      "[56:09.440 --> 56:16.000]  one slide and then they're all going to and now this guy is going to move here and now I will\n",
      "[56:16.000 --> 56:22.640]  pay one slide for each one here. Now this is the simplified version of it. Once you go to the complete\n",
      "[56:22.640 --> 56:27.680]  version of it, you have some extra slides within the path, but you can also count those.\n",
      "[56:29.440 --> 56:37.920]  Okay. So, but does this mean that so does this mean that at a particular vertex we are placing\n",
      "[56:38.000 --> 56:48.320]  the token at most once? In this case, yes. Okay. In this case, yes. Okay. So this problem should be hard\n",
      "[56:48.320 --> 56:57.520]  even if we bound the number of times tokens can be moved to our vertex, right? Yes. Okay.\n",
      "[56:57.520 --> 57:02.720]  Yes. So here in this case, yes. Absolutely. Okay. Thanks.\n",
      "[57:03.440 --> 57:13.360]  So, Akansha, I have a remark about your question. So if a vertex cannot get a token to I's,\n",
      "[57:14.000 --> 57:18.080]  then it somehow seems to be selecting disjoint independent sets.\n",
      "[57:20.800 --> 57:25.760]  A sequence of them and that may have some bearing on coloring. Just a top level top.\n",
      "[57:25.920 --> 57:35.520]  Okay. So actually for the list, the W hardness case that I'm going to present it, it is exactly\n",
      "[57:35.520 --> 57:41.360]  the case, right? So we are not allowed to move the token like twice on the same vertex.\n",
      "[57:43.280 --> 57:49.360]  Yeah. So I didn't get your point of being so getting this disjoint independence, it's actually\n",
      "[57:50.080 --> 57:56.000]  because if you think of it from the way I thought about it, right, that you are actually trying to find\n",
      "[57:56.000 --> 58:01.680]  a path in a large graph where every vertex corresponds to an independent set and you move from one\n",
      "[58:01.680 --> 58:10.960]  independent set to another. But we can only move from one independent set to the other if the change\n",
      "[58:10.960 --> 58:24.400]  is like in case of tokens sliding, it's one probably. Yeah. So it looks to be that you're asking for\n",
      "[58:26.160 --> 58:32.560]  a collection of independent sets which are vertex disjoint. If the token sequence of independence\n",
      "[58:32.560 --> 58:39.600]  sets which are vertex disjoint. So if I may, I think I think Akansha's question would be more\n",
      "[58:39.600 --> 58:46.080]  relevant in a place where we don't have a monotone sequence, meaning a sequence. So we need a\n",
      "[58:46.080 --> 58:51.920]  version of the problem or some cases of the problem where a vertex has to be visited multiple times\n",
      "[58:53.520 --> 58:59.040]  to find solutions. And that is known to be the case for some versions or some statements of the\n",
      "[58:59.040 --> 59:05.760]  problem. And in fact, Akansha, so this was the crucial difference between piece-based completeness\n",
      "[59:05.760 --> 59:12.480]  and NP completeness of sliding versus jumping in bipartite graphs. So it was because we were\n",
      "[59:12.480 --> 59:19.760]  able to show that no vertex will be visited more than once. Okay. And the other problem. So that's\n",
      "[59:19.760 --> 59:24.560]  why it's definitely an interesting question to pose, but you have to be careful in what context you\n",
      "[59:24.560 --> 59:35.520]  pose it. Great. I don't know if that kind of settles, answers your question. Yes, yes, it does.\n",
      "[59:36.080 --> 59:39.120]  All right. Thanks. You're welcome.\n",
      "[59:46.960 --> 59:54.320]  Any more questions?\n",
      "[01:00:05.760 --> 01:00:26.480]  I guess not. Yeah, I don't think that I know more questions. I will just once again announce the\n",
      "[01:00:26.480 --> 01:00:32.320]  parameterized and go to the 301 workshop, which is going to happen in December in the link has been\n",
      "[01:00:32.400 --> 01:00:38.560]  posted once again in the chat. Some advanced topics in parameterized complexity will be discussed.\n",
      "[01:00:38.560 --> 01:00:46.720]  Those interested can have a look and register for it. And yeah, if there are some more questions,\n",
      "[01:00:46.720 --> 01:00:54.320]  please ask away.\n",
      "[01:01:02.320 --> 01:01:20.480]  So anyone can register for the school? Yes, here's anyone can.\n",
      "[01:01:20.480 --> 01:01:27.200]  Yeah, it's free and it's online and yeah, it's open to everyone.\n",
      "[01:01:27.840 --> 01:01:32.000]  Awesome. So I can share it with my students as well. Of course, of course, please do. Yeah, that's\n",
      "[01:01:32.000 --> 01:01:38.960]  a good one. And we assume some basic understanding of parameterized algorithms, but we have already shared\n",
      "[01:01:38.960 --> 01:01:45.760]  a link on the page where students can go and go through some previous lectures in parameterized\n",
      "[01:01:45.760 --> 01:01:50.320]  algorithms if they wish to just brace up or revise stuff.\n",
      "[01:01:57.680 --> 01:02:05.520]  All right, so I guess, okay, I don't think there are any more questions. So I think it's a good time\n",
      "[01:02:05.520 --> 01:02:12.640]  to wrap up. So thank you once again for the summit for agreeing to give the talk. It was really nice\n",
      "[01:02:12.640 --> 01:02:17.600]  to have you and it was really good to have something different than what we usually hear in every\n",
      "[01:02:17.600 --> 01:02:23.520]  parameterized complexity talk, at least most of them. So and yeah, these are really interesting\n",
      "[01:02:23.520 --> 01:02:29.760]  problems. Do you think of one? And thank you to the audience for being with us. And that's it for\n",
      "[01:02:29.760 --> 01:02:35.040]  today's wrap up. See you all next week. Thank you. Bye. Thank you. Bye.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "# Load the model (tiny, base, small, medium, or large)\n",
    "model = whisper.load_model(\"base\",device=\"cuda\")\n",
    "\n",
    "# Transcribe the audio file\n",
    "result = model.transcribe(\"audio.wav\", verbose=True)\n",
    "\n",
    "# Save as SRT\n",
    "with open(\"transcription.srt\", \"w\", encoding=\"utf-8\") as srt_file:\n",
    "    for i, segment in enumerate(result[\"segments\"]):\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        text = segment[\"text\"].strip()\n",
    "\n",
    "        # Format timestamps\n",
    "        def format_timestamp(seconds):\n",
    "            h = int(seconds // 3600)\n",
    "            m = int((seconds % 3600) // 60)\n",
    "            s = seconds % 60\n",
    "            return f\"{h:02}:{m:02}:{s:06.3f}\".replace('.', ',')\n",
    "\n",
    "        srt_file.write(f\"{i + 1}\\n\")\n",
    "        srt_file.write(f\"{format_timestamp(start)} --> {format_timestamp(end)}\\n\")\n",
    "        srt_file.write(f\"{text}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb89f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Load the model (options: 'tiny', 'base', 'small', 'medium', 'large-v2')\n",
    "model = WhisperModel(\"base\", device=\"cpu\", compute_type=\"int8_float32\")\n",
    "\n",
    "# Transcribe the audio file\n",
    "segments, info = model.transcribe(\"audio.wav\", beam_size=5, language=\"en\")\n",
    "\n",
    "# Save as SRT\n",
    "with open(\"transcription2.srt\", \"w\", encoding=\"utf-8\") as srt_file:\n",
    "    for i, segment in enumerate(segments):\n",
    "        start = segment.start\n",
    "        end = segment.end\n",
    "        text = segment.text.strip()\n",
    "\n",
    "        # Format timestamps\n",
    "        def format_timestamp(seconds):\n",
    "            h = int(seconds // 3600)\n",
    "            m = int((seconds % 3600) // 60)\n",
    "            s = seconds % 60\n",
    "            return f\"{h:02}:{m:02}:{s:06.3f}\".replace('.', ',')\n",
    "\n",
    "        srt_file.write(f\"{i + 1}\\n\")\n",
    "        srt_file.write(f\"{format_timestamp(start)} --> {format_timestamp(end)}\\n\")\n",
    "        srt_file.write(f\"{text}\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
